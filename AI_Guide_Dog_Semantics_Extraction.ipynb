{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG_YUKHCRLbw"
      },
      "source": [
        "# Semantic Segmentation Demo\n",
        "\n",
        "This is a notebook for running the benchmark semantic segmentation network from the the [ADE20K MIT Scene Parsing Benchchmark](http://sceneparsing.csail.mit.edu/).\n",
        "\n",
        "The code for this notebook is available here\n",
        "https://github.com/CSAILVision/semantic-segmentation-pytorch/tree/master/notebooks\n",
        "\n",
        "It can be run on Colab at this URL https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sZVxxIhoz1Wr",
        "outputId": "c033bcb9-f65e-495f-a568-3145ee68c0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "an = [[-0.004716190509498119, -0.0015442944131791592, 0.0015981390606611967, -2.1806214135722257e-05, 7.439052569679917e-05, -1.0732343071140347e-05, 1.0], [0.02309301868081093, 0.0023342808708548546, 0.0506298653781414, 3.401018329896033e-05, -0.0019222927512601016, 0.0021084910258650775, 0.9999959468841552], [0.053202029317617416, 0.03648602962493896, 0.4865258038043976, -0.0004901473876088858, -0.0041520525701344005, -0.006891436409205198, 0.9999675154685974], [0.06836142390966415, -0.03956856951117516, 1.286145567893982, -0.0009973392589017747, 0.005531878676265478, -0.002320240251719952, 0.9999815821647645], [0.0203765332698822, -0.10057416558265686, 2.4121177196502686, -0.0005310514243319632, 0.01537159364670515, 0.001361840753816068, 0.9998807907104493], [0.045108165591955185, -0.14525069296360016, 3.55315899848938, 7.468684634659438e-05, 0.005704524926841259, 0.00235587265342474, 0.9999809861183167], [0.13488395512104034, -0.1963791251182556, 4.689173221588135, -0.0004860237531829625, -0.00040600186912342906, -0.004166984930634499, 0.9999911189079284], [0.1149468645453453, -0.2464183568954468, 5.962574481964111, -0.0004622219421435148, 0.00878873560577631, 0.0006996745360083878, 0.9999610781669616], [0.09449680894613266, -0.2945286333560944, 7.2549214363098145, -0.0008242461481131613, 0.01497135404497385, 0.002424252685159445, 0.9998847246170046], [0.1734085828065872, -0.35626524686813354, 8.757527351379393, 0.0006751691689714788, 0.0007519553182646632, -0.007941267453134062, 0.9999679923057555]]"
      ],
      "metadata": {
        "id": "wQVlXw_ARJkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(an)"
      ],
      "metadata": {
        "id": "L7jeBeVzRLJD",
        "outputId": "7e9a5d1d-39b3-4f14-b7ee-89cf58bb2fba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1WI9ZhkRLb0"
      },
      "source": [
        "### Environment Setup\n",
        "\n",
        "First, download the code and pretrained models if we are on colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKNrL-7iRLb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efae51eb-cf83-4c84-b8d5-d4acb109e664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From https://github.com/CSAILVision/semantic-segmentation-pytorch\n",
            " * branch            master     -> FETCH_HEAD\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Colab-specific setup\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "pip install yacs 2>&1 >> install.log\n",
        "git init 2>&1 >> install.log\n",
        "git remote add origin https://github.com/CSAILVision/semantic-segmentation-pytorch.git 2>> install.log\n",
        "git pull origin master 2>&1 >> install.log\n",
        "DOWNLOAD_ONLY=1 ./demo_test.sh 2>> install.log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CSAILVision/semantic-segmentation-pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd4FRRUmsrqc",
        "outputId": "1fb07c70-9881-43e4-9453-f0ef11aaaeb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'semantic-segmentation-pytorch'...\n",
            "remote: Enumerating objects: 1170, done.\u001b[K\n",
            "remote: Total 1170 (delta 0), reused 0 (delta 0), pack-reused 1170 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1170/1170), 5.04 MiB | 7.78 MiB/s, done.\n",
            "Resolving deltas: 100% (707/707), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLW4DaamRLb3"
      },
      "source": [
        "## Imports and utility functions\n",
        "\n",
        "We need pytorch, numpy, and the code for the segmentation model.  And some utilities for visualizing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUVvmZObRLb4"
      },
      "outputs": [],
      "source": [
        "# System libs\n",
        "import os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\n",
        "# Our libs\n",
        "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
        "from mit_semseg.utils import colorEncode\n",
        "\n",
        "# colors = scipy.io.loadmat('data/color150.mat')['colors']\n",
        "# names = {}\n",
        "\n",
        "# with open('data/object150_info.csv') as f:\n",
        "#     reader = csv.reader(f)\n",
        "#     next(reader)\n",
        "#     for row in reader:\n",
        "#         names[int(row[0])] = row[5].split(\";\")[0]\n",
        "\n",
        "# def visualize_result(img, pred, index=None):\n",
        "#     # filter prediction class if requested\n",
        "#     if index is not None:\n",
        "#         pred = pred.copy()\n",
        "#         pred[pred != index] = -1\n",
        "#         print(f'{names[index+1]}:')\n",
        "\n",
        "#     # colorize prediction\n",
        "#     pred_color = colorEncode(pred, colors).astype(numpy.uint8)\n",
        "\n",
        "#     # aggregate images and save\n",
        "#     im_vis = numpy.concatenate((img, pred_color), axis=1)\n",
        "#     display(PIL.Image.fromarray(im_vis))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_QJItFqs3bn",
        "outputId": "4d78ede6-88e6-4e93-8a77-a1790664d066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mckpt\u001b[0m/          \u001b[01;32mdownload_ADE20K.sh\u001b[0m*  LICENSE      requirements.txt                \u001b[01;34mteaser\u001b[0m/\n",
            "\u001b[01;34mconfig\u001b[0m/        eval_multipro.py     \u001b[01;34mmit_semseg\u001b[0m/  \u001b[01;34msample_data\u001b[0m/                    test.py\n",
            "\u001b[01;34mdata\u001b[0m/          eval.py              \u001b[01;34mnotebooks\u001b[0m/   \u001b[01;34msemantic-segmentation-pytorch\u001b[0m/  train.py\n",
            "\u001b[01;32mdemo_test.sh\u001b[0m*  install.log          README.md    setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceq0V_ifRLb5"
      },
      "source": [
        "## Loading the segmentation model\n",
        "\n",
        "Here we load a pretrained segmentation model.  Like any pytorch model, we can call it like a function, or examine the parameters in all the layers.\n",
        "\n",
        "After loading, we put it on the GPU.  And since we are doing inference, not training, we put the model in eval mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6nEb5y-RLb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada514d2-4b06-4a58-bc33-40c0b505e7d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights for net_encoder\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mit_semseg/models/models.py:109: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights for net_decoder\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mit_semseg/models/models.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SegmentationModule(\n",
              "  (encoder): ResnetDilated(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "    (relu2): ReLU(inplace=True)\n",
              "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn3): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "    (relu3): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): PPMDeepsup(\n",
              "    (ppm): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=1)\n",
              "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=2)\n",
              "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=3)\n",
              "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=6)\n",
              "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (cbr_deepsup): Sequential(\n",
              "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv_last): Sequential(\n",
              "      (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout2d(p=0.1, inplace=False)\n",
              "      (4): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (conv_last_deepsup): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (dropout_deepsup): Dropout2d(p=0.1, inplace=False)\n",
              "  )\n",
              "  (crit): NLLLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Network Builders\n",
        "net_encoder = ModelBuilder.build_encoder(\n",
        "    arch='resnet50dilated',\n",
        "    fc_dim=2048,\n",
        "    weights='ckpt/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth')\n",
        "net_decoder = ModelBuilder.build_decoder(\n",
        "    arch='ppm_deepsup',\n",
        "    fc_dim=2048,\n",
        "    num_class=150,\n",
        "    weights='ckpt/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth',\n",
        "    use_softmax=True)\n",
        "\n",
        "crit = torch.nn.NLLLoss(ignore_index=-1)\n",
        "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
        "segmentation_module.eval()\n",
        "segmentation_module.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UhjERUcRLb6"
      },
      "source": [
        "## Segmentation Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtSD9pRYRLb7"
      },
      "outputs": [],
      "source": [
        "# Load and normalize one image as a singleton tensor batch\n",
        "pil_to_tensor = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
        "        std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pil_image = PIL.Image.open('ADE_val_00001519.jpg').convert('RGB')\n",
        "img_original = numpy.array(pil_image)\n",
        "img_data = pil_to_tensor(pil_image)\n",
        "singleton_batch = {'img_data': img_data[None].cuda()}\n",
        "output_size = img_data.shape[1:]"
      ],
      "metadata": {
        "id": "YYA91phJ65cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "image_paths = ['ADE_val_00001519.jpg', 'ADE_val_00001519.jpg', 'ADE_val_00001519.jpg']\n",
        "images = [PIL.Image.open(img_path).convert('RGB') for img_path in image_paths]\n",
        "img_originals = [np.array(img) for img in images]\n",
        "\n",
        "# Preprocess images and create batch\n",
        "img_data = [pil_to_tensor(img)[None] for img in images]  # Normalize and add batch dimension\n",
        "batch_data = torch.cat(img_data).cuda()  # Create a batch of images\n",
        "\n",
        "# Run segmentation\n",
        "segmentation_module.eval()\n",
        "with torch.no_grad():\n",
        "    scores = segmentation_module({'img_data': batch_data}, segSize=output_size)\n",
        "\n",
        "# Get predicted segmentation masks\n",
        "_, preds = torch.max(scores, dim=1)\n",
        "pred_masks = preds.cpu().numpy()"
      ],
      "metadata": {
        "id": "KVDUWRt0WK6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK-Hrv-4RLb7"
      },
      "source": [
        "## Run the Model\n",
        "\n",
        "Finally we just pass the test image to the segmentation model.\n",
        "\n",
        "The segmentation model is coded as a function that takes a dictionary as input, because it wants to know both the input batch image data as well as the desired output segmentation resolution.  We ask for full resolution output.\n",
        "\n",
        "Then we use the previously-defined visualize_result function to render the segmentation map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "aT2bmfZkRLb8"
      },
      "outputs": [],
      "source": [
        "# Run the segmentation at the highest resolution.\n",
        "with torch.no_grad():\n",
        "    scores = segmentation_module(singleton_batch, segSize=output_size)\n",
        "\n",
        "# Get the predicted scores for each pixel\n",
        "_, pred = torch.max(scores, dim=1)\n",
        "pred = pred.cpu()[0].numpy()\n",
        "# visualize_result(img_original, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX4sThIpRLb9"
      },
      "source": [
        "## Showing classes individually\n",
        "\n",
        "To see which colors are which, here we visualize individual classes, one at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1x54BXzRLb9"
      },
      "outputs": [],
      "source": [
        "# # Top classes in answer\n",
        "# predicted_classes = numpy.bincount(pred.flatten()).argsort()[::-1]\n",
        "# for c in predicted_classes[:15]:\n",
        "#     visualize_result(img_original, pred, c)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t00lVh8IVOuZ",
        "outputId": "e3b42613-d992-496c-8ecc-e2014b9d4813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder"
      ],
      "metadata": {
        "id": "iXPFX2SkXFsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MaskAutoencoder(nn.Module):\n",
        "    def __init__(self, height, width, k):\n",
        "        super(MaskAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Calculating dimensions after downsampling\n",
        "        def conv_output_size(input_size, kernel_size=3, stride=2, padding=1):\n",
        "            return (input_size + 2*padding - kernel_size) // stride + 1\n",
        "\n",
        "        h_after_conv1 = conv_output_size(height)\n",
        "        w_after_conv1 = conv_output_size(width)\n",
        "        h_after_conv2 = conv_output_size(h_after_conv1)\n",
        "        w_after_conv2 = conv_output_size(w_after_conv1)\n",
        "\n",
        "        self.encoded_dim = h_after_conv2 * w_after_conv2 * 64\n",
        "        #self.encoded_dim = (height // 4) * (width // 4) * 64\n",
        "        self.projection = nn.Linear(self.encoded_dim, k)\n",
        "\n",
        "        # Decoder components\n",
        "        self.unprojection = nn.Linear(k, self.encoded_dim)\n",
        "        self.unflatten = nn.Unflatten(1, (64, h_after_conv2, w_after_conv2))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            #nn.PixelShuffle(1), # Not fix input and reconstructed size mismtach\n",
        "            nn.Sigmoid()  # Sigmoid activation for pixel values in [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoding\n",
        "        features = self.encoder(x)\n",
        "\n",
        "        flattened = self.flatten(features)\n",
        "\n",
        "        projected = self.projection(flattened)\n",
        "\n",
        "        # Decoding\n",
        "        unprojected = self.unprojection(projected)\n",
        "\n",
        "        unflattened = self.unflatten(unprojected)\n",
        "\n",
        "        reconstructed = self.decoder(unflattened)\n",
        "        reconstructed = reconstructed[:, :, :x.shape[2], :x.shape[3]] # align output size with input size\n",
        "\n",
        "        return projected, reconstructed\n",
        "\n",
        "# Example reconstruction loss calculation\n",
        "def reconstruction_loss(original, reconstructed):\n",
        "    loss_fn = nn.MSELoss()  # Mean Squared Error for pixel-wise comparison\n",
        "    return loss_fn(reconstructed, original)\n"
      ],
      "metadata": {
        "id": "_R0lHq7qXE-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(270 // 4) * (480 // 4) * 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZPDTatxgZA1",
        "outputId": "1e5127fa-6622-4123-8897-6942e33d88d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "514560"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "height, width = pred.shape  # Example height and width\n",
        "k = 648\n",
        "\n",
        "simple_autoencoder = MaskAutoencoder(height, width, k)\n",
        "simple_autoencoder = simple_autoencoder.cuda()\n",
        "\n",
        "pred_tensor = torch.tensor(pred, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, height, width)\n",
        "pred_tensor = pred_tensor.cuda()\n",
        "\n",
        "# Forward pass through the simple autoencoder\n",
        "encoded_mask, reconstructed_mask = simple_autoencoder(pred_tensor)\n",
        "\n",
        "# Calculate reconstruction loss\n",
        "reconstruction_loss = nn.MSELoss()\n",
        "loss = reconstruction_loss(reconstructed_mask, pred_tensor)\n",
        "\n",
        "print(\"Encoded Mask Shape:\", encoded_mask.shape)\n",
        "print(\"Reconstructed Mask Shape:\", reconstructed_mask.shape)\n",
        "print(\"Reconstruction Loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH5SPcz9XVf-",
        "outputId": "e52910c2-0efb-4f29-f69c-0e52c813e902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Mask Shape: torch.Size([1, 648])\n",
            "Reconstructed Mask Shape: torch.Size([1, 1, 512, 768])\n",
            "Reconstruction Loss: 833.5474853515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_image_and_get_loss(image_path, segmentation_module, autoencoder, normalization_params, device='cuda'):\n",
        "    # Load and normalize the image\n",
        "    pil_to_tensor = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
        "        std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
        "    ])\n",
        "\n",
        "    pil_image = PIL.Image.open(image_path).convert('RGB')\n",
        "    img_original = numpy.array(pil_image)\n",
        "    img_data = pil_to_tensor(pil_image)\n",
        "    singleton_batch = {'img_data': img_data[None].cuda()}\n",
        "    output_size = img_data.shape[1:]\n",
        "\n",
        "\n",
        "    # Run segmentation at the highest resolution\n",
        "    with torch.no_grad():\n",
        "        scores = segmentation_module(singleton_batch, segSize=output_size)\n",
        "\n",
        "    # Get the predicted scores for each pixel\n",
        "    _, pred = torch.max(scores, dim=1)\n",
        "    pred = pred.cpu()[0].numpy()\n",
        "    pred_tensor = torch.tensor(pred, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, height, width)\n",
        "    pred_tensor = pred_tensor.cuda()\n",
        "\n",
        "    # Forward pass through the autoencoder\n",
        "    encoded_mask, reconstructed_mask = autoencoder(pred_tensor)\n",
        "\n",
        "    # Calculate reconstruction loss\n",
        "    reconstruction_loss = nn.MSELoss()\n",
        "    loss = reconstruction_loss(reconstructed_mask, pred_tensor)\n",
        "\n",
        "    return encoded_mask, loss.item()\n",
        "\n",
        "# Example usage\n",
        "normalization_params = {\n",
        "    'mean': [0.485, 0.456, 0.406],\n",
        "    'std': [0.229, 0.224, 0.225]\n",
        "}\n",
        "\n",
        "# Assuming `segmentation_module` and `simple_autoencoder` are defined and initialized\n",
        "image_path = 'ADE_val_00001519.jpg'\n",
        "encoded_mask, reconstruction_loss = process_image_and_get_loss(image_path, segmentation_module, simple_autoencoder, normalization_params)\n",
        "\n",
        "print(\"Encoded Mask Shape:\", encoded_mask.shape)\n",
        "print(\"Reconstruction Loss:\", reconstruction_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA0Jb5hQXczn",
        "outputId": "3a9cbeb2-0f7f-4261-d0cc-403952d99df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(512, 768)\n",
            "torch.Size([1, 1, 512, 768])\n",
            "Encoded Mask Shape: torch.Size([1, 648])\n",
            "Reconstruction Loss: 837.4664916992188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WOzs2qAZcQs",
        "outputId": "9dd0fcf6-6cae-4b32-cc4a-3e9b79b7a315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load video from dropbox"
      ],
      "metadata": {
        "id": "iQNFs81Nttai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O dropbox_files.zip \"https://www.dropbox.com/scl/fo/0q61ds2qxx6exwem6qbzc/AEMYOyGeSoJwJNIT1Y_mzlU?rlkey=krsdnjjby90kzo079szb8zy7g&st=1hucb4b9&dl=1\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTBsfneEtsFS",
        "outputId": "0191c19b-275b-4690-e30e-361c78c37a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-20 17:58:41--  https://www.dropbox.com/scl/fo/0q61ds2qxx6exwem6qbzc/AEMYOyGeSoJwJNIT1Y_mzlU?rlkey=krsdnjjby90kzo079szb8zy7g&st=1hucb4b9&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc889812666912e9c14f0d4a3092.dl.dropboxusercontent.com/zip_download_get/CCLHm-0OESP6yLjKB4mi_PTgUCF_LjXZRmLjZwKyGNlvQLGDYBarrEtSLbzx4Nf43cKeZ-2KcEJhe2425bYvOAFJj0q46GrBBwaaYUwsHjAmyA# [following]\n",
            "--2024-11-20 17:58:42--  https://uc889812666912e9c14f0d4a3092.dl.dropboxusercontent.com/zip_download_get/CCLHm-0OESP6yLjKB4mi_PTgUCF_LjXZRmLjZwKyGNlvQLGDYBarrEtSLbzx4Nf43cKeZ-2KcEJhe2425bYvOAFJj0q46GrBBwaaYUwsHjAmyA\n",
            "Resolving uc889812666912e9c14f0d4a3092.dl.dropboxusercontent.com (uc889812666912e9c14f0d4a3092.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc889812666912e9c14f0d4a3092.dl.dropboxusercontent.com (uc889812666912e9c14f0d4a3092.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1806307466 (1.7G) [application/zip]\n",
            "Saving to: ‘dropbox_files.zip’\n",
            "\n",
            "dropbox_files.zip   100%[===================>]   1.68G   104MB/s    in 17s     \n",
            "\n",
            "2024-11-20 17:59:00 (102 MB/s) - ‘dropbox_files.zip’ saved [1806307466/1806307466]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"dropbox_files.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dropbox_downloads\")\n",
        "print(\"Files extracted to /content/dropbox_downloads\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhlorbzbutST",
        "outputId": "b6ad9b35-7ecb-4ecc-a47d-f4b3bebd27d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/dropbox_downloads\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define source directory and destination folder for .mp4 files\n",
        "source_dir = \"/content/dropbox_downloads/VirtualGuideDogLib\"\n",
        "mp4_output_dir = \"/content/mp4_files\"\n",
        "os.makedirs(mp4_output_dir, exist_ok=True)\n",
        "\n",
        "# Walk through the directory tree and copy all .mp4 files to the output directory\n",
        "for root, dirs, files in os.walk(source_dir):\n",
        "    print(root, dirs, files)\n",
        "    for file in files:\n",
        "        if file.endswith(\".mp4\"):\n",
        "            full_path = os.path.join(root, file)\n",
        "            shutil.copy(full_path, mp4_output_dir)\n",
        "            print(f\"Copied: {full_path}\")\n",
        "\n",
        "print(f\"All .mp4 files are now in: {mp4_output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5MksmlpvRWH",
        "outputId": "bd3fb059-0134-40e3-f1c5-4dd3db8cd443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dropbox_downloads/VirtualGuideDogLib ['test'] []\n",
            "/content/dropbox_downloads/VirtualGuideDogLib/test ['2022-04-04T12:36:21.745Z', '2022-04-04T12:43:51.211Z', '2022-04-04T13:01:31.083Z', '2022-04-04T13:15:19.655Z', '2022-04-04T12:14:43.949Z', '2022-04-04T12:51:41.445Z', '2022-04-04T12:56:37.203Z'] []\n",
            "/content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:36:21.745Z [] ['2022-04-04T12:36:21.745Z.csv', '2022-04-04T12:36:21.745Z.mp4']\n",
            "Copied: /content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:36:21.745Z/2022-04-04T12:36:21.745Z.mp4\n",
            "/content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:43:51.211Z [] ['2022-04-04T12:43:51.211Z.csv', '2022-04-04T12:43:51.211Z.mp4']\n",
            "Copied: /content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:43:51.211Z/2022-04-04T12:43:51.211Z.mp4\n",
            "/content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T13:01:31.083Z [] ['2022-04-04T13:01:31.083Z.mp4', '2022-04-04T13:01:31.083Z.csv']\n",
            "Copied: /content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T13:01:31.083Z/2022-04-04T13:01:31.083Z.mp4\n",
            "/content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T13:15:19.655Z [] ['2022-04-04T13:15:19.655Z.mp4', '2022-04-04T13:15:19.655Z.csv']\n",
            "Copied: /content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T13:15:19.655Z/2022-04-04T13:15:19.655Z.mp4\n",
            "/content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:14:43.949Z [] ['2022-04-04T12:14:43.949Z.mp4', '2022-04-04T12:14:43.949Z.csv']\n",
            "Copied: /content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:14:43.949Z/2022-04-04T12:14:43.949Z.mp4\n",
            "/content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:51:41.445Z [] ['2022-04-04T12:51:41.445Z.mp4', '2022-04-04T12:51:41.445Z.csv']\n",
            "Copied: /content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:51:41.445Z/2022-04-04T12:51:41.445Z.mp4\n",
            "/content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:56:37.203Z [] ['2022-04-04T12:56:37.203Z.mp4', '2022-04-04T12:56:37.203Z.csv']\n",
            "Copied: /content/dropbox_downloads/VirtualGuideDogLib/test/2022-04-04T12:56:37.203Z/2022-04-04T12:56:37.203Z.mp4\n",
            "All .mp4 files are now in: /content/mp4_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYsStDscwkOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O dropbox_files2.zip \"https://www.dropbox.com/scl/fo/d3663gx12my7sfzsmzbp4/APdg9wKx5wA7pvlMIu44Z-M?rlkey=iazskxeadgqrt5oqweb831ka3&e=1&st=p6sehk48&dl=1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHDoPX4MwNQ7",
        "outputId": "7b1e56fa-94e3-440e-af96-71c5e2f08382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-20 18:06:33--  https://www.dropbox.com/scl/fo/d3663gx12my7sfzsmzbp4/APdg9wKx5wA7pvlMIu44Z-M?rlkey=iazskxeadgqrt5oqweb831ka3&e=1&st=p6sehk48&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc9bccc537c2c6aed7f95f014ddc.dl.dropboxusercontent.com/zip_download_get/CCLmLQDUytZDZluWVExNwi3RnOAN3bXJXaROf4N3kSLMv0nBEHt8ryczOOkMGmMfUWAgbUlFopQIBX9Vhr5dChLRrvnUuPUbcZ7uqDj1iJR4uQ# [following]\n",
            "--2024-11-20 18:06:35--  https://uc9bccc537c2c6aed7f95f014ddc.dl.dropboxusercontent.com/zip_download_get/CCLmLQDUytZDZluWVExNwi3RnOAN3bXJXaROf4N3kSLMv0nBEHt8ryczOOkMGmMfUWAgbUlFopQIBX9Vhr5dChLRrvnUuPUbcZ7uqDj1iJR4uQ\n",
            "Resolving uc9bccc537c2c6aed7f95f014ddc.dl.dropboxusercontent.com (uc9bccc537c2c6aed7f95f014ddc.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc9bccc537c2c6aed7f95f014ddc.dl.dropboxusercontent.com (uc9bccc537c2c6aed7f95f014ddc.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1406980214 (1.3G) [application/zip]\n",
            "Saving to: ‘dropbox_files2.zip’\n",
            "\n",
            "dropbox_files2.zip  100%[===================>]   1.31G  71.2MB/s    in 21s     \n",
            "\n",
            "2024-11-20 18:06:56 (63.7 MB/s) - ‘dropbox_files2.zip’ saved [1406980214/1406980214]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(\"dropbox_files2.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dropbox_downloads2\")\n",
        "print(\"Files extracted to /content/dropbox_downloads2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM1K6F8awWBx",
        "outputId": "fac7c2b6-ea50-4d0e-ee35-27fa15ce710c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/dropbox_downloads2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /content/dropbox_downloads2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDGkiQuzu2Lm",
        "outputId": "2ac6125b-c44f-4440-b9ba-d1b324a5b385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m2022-04-04T18:41:15.478Z\u001b[0m/  \u001b[01;34m2022-04-04T19:22:45.749Z\u001b[0m/  \u001b[01;34m2022-09-07T17:48:53.625Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T18:47:26.422Z\u001b[0m/  \u001b[01;34m2022-04-04T19:23:24.989Z\u001b[0m/  \u001b[01;34m2022-09-07T17:49:31.370Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T18:48:45.704Z\u001b[0m/  \u001b[01;34m2022-04-04T19:24:10.208Z\u001b[0m/  \u001b[01;34m2022-09-07T17:50:02.923Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T18:58:12.589Z\u001b[0m/  \u001b[01;34m2022-04-04T19:28:21.899Z\u001b[0m/  \u001b[01;34m2022-09-07T17:50:31.458Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T18:59:05.875Z\u001b[0m/  \u001b[01;34m2022-04-04T19:28:40.596Z\u001b[0m/  \u001b[01;34m2022-09-07T17:51:24.478Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:01:16.917Z\u001b[0m/  \u001b[01;34m2022-04-04T19:28:57.167Z\u001b[0m/  \u001b[01;34m2022-09-07T18:15:13.682Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:02:28.038Z\u001b[0m/  \u001b[01;34m2022-09-07T17:42:05.175Z\u001b[0m/  \u001b[01;34m2022-09-07T18:15:32.232Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:04:56.768Z\u001b[0m/  \u001b[01;34m2022-09-07T17:43:08.884Z\u001b[0m/  \u001b[01;34m2022-09-07T18:15:43.550Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:06:29.642Z\u001b[0m/  \u001b[01;34m2022-09-07T17:43:16.918Z\u001b[0m/  \u001b[01;34m2022-09-07T18:16:01.302Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:07:55.583Z\u001b[0m/  \u001b[01;34m2022-09-07T17:43:39.620Z\u001b[0m/  \u001b[01;34m2022-09-07T18:21:23.281Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:09:43.644Z\u001b[0m/  \u001b[01;34m2022-09-07T17:44:43.991Z\u001b[0m/  \u001b[01;34m2022-09-07T18:21:40.015Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:10:28.196Z\u001b[0m/  \u001b[01;34m2022-09-07T17:45:09.377Z\u001b[0m/  \u001b[01;34m2022-09-07T18:23:12.931Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:16:45.412Z\u001b[0m/  \u001b[01;34m2022-09-07T17:46:38.914Z\u001b[0m/  \u001b[01;34m2022-09-07T18:23:39.867Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:17:50.959Z\u001b[0m/  \u001b[01;34m2022-09-07T17:47:02.517Z\u001b[0m/  \u001b[01;34m2022-09-07T18:24:13.386Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:18:52.214Z\u001b[0m/  \u001b[01;34m2022-09-07T17:47:34.402Z\u001b[0m/  \u001b[01;34m2022-09-07T18:24:39.188Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:19:33.984Z\u001b[0m/  \u001b[01;34m2022-09-07T17:48:06.905Z\u001b[0m/  \u001b[01;34m2022-09-07T18:25:51.421Z\u001b[0m/\n",
            "\u001b[01;34m2022-04-04T19:21:41.445Z\u001b[0m/  \u001b[01;34m2022-09-07T17:48:18.039Z\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define source directory and destination folder for .mp4 files\n",
        "source_dir = \"/content/dropbox_downloads2/\"\n",
        "mp4_output_dir = \"/content/mp4_files\"\n",
        "os.makedirs(mp4_output_dir, exist_ok=True)\n",
        "\n",
        "# Walk through the directory tree and copy all .mp4 files to the output directory\n",
        "for root, dirs, files in os.walk(source_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".mp4\"):\n",
        "            full_path = os.path.join(root, file)\n",
        "            shutil.copy(full_path, mp4_output_dir)\n",
        "            print(f\"Copied: {full_path}\")\n",
        "\n",
        "print(f\"All .mp4 files are now in: {mp4_output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU2fT7YVwp__",
        "outputId": "7b84d386-e496-4f18-9a10-3b742f00f3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied: /content/dropbox_downloads2/2022-04-04T19:09:43.644Z/2022-04-04T19:09:43.644Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:28:21.899Z/2022-04-04T19:28:21.899Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:10:28.196Z/2022-04-04T19:10:28.196Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:17:50.959Z/2022-04-04T19:17:50.959Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:23:39.867Z/2022-09-07T18:23:39.867Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:25:51.421Z/2022-09-07T18:25:51.421Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:23:12.931Z/2022-09-07T18:23:12.931Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:51:24.478Z/2022-09-07T17:51:24.478Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:16:45.412Z/2022-04-04T19:16:45.412Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:04:56.768Z/2022-04-04T19:04:56.768Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:19:33.984Z/2022-04-04T19:19:33.984Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:45:09.377Z/2022-09-07T17:45:09.377Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:21:40.015Z/2022-09-07T18:21:40.015Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T18:41:15.478Z/2022-04-04T18:41:15.478Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:01:16.917Z/2022-04-04T19:01:16.917Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:28:57.167Z/2022-04-04T19:28:57.167Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:50:31.458Z/2022-09-07T17:50:31.458Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:47:02.517Z/2022-09-07T17:47:02.517Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:48:06.905Z/2022-09-07T17:48:06.905Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:02:28.038Z/2022-04-04T19:02:28.038Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T18:48:45.704Z/2022-04-04T18:48:45.704Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:24:10.208Z/2022-04-04T19:24:10.208Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:47:34.402Z/2022-09-07T17:47:34.402Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:50:02.923Z/2022-09-07T17:50:02.923Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:46:38.914Z/2022-09-07T17:46:38.914Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:15:13.682Z/2022-09-07T18:15:13.682Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:21:23.281Z/2022-09-07T18:21:23.281Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:48:18.039Z/2022-09-07T17:48:18.039Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:18:52.214Z/2022-04-04T19:18:52.214Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T18:47:26.422Z/2022-04-04T18:47:26.422Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:43:08.884Z/2022-09-07T17:43:08.884Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:06:29.642Z/2022-04-04T19:06:29.642Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:07:55.583Z/2022-04-04T19:07:55.583Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:43:16.918Z/2022-09-07T17:43:16.918Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:22:45.749Z/2022-04-04T19:22:45.749Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:23:24.989Z/2022-04-04T19:23:24.989Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T18:58:12.589Z/2022-04-04T18:58:12.589Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:15:43.550Z/2022-09-07T18:15:43.550Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:42:05.175Z/2022-09-07T17:42:05.175Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:24:13.386Z/2022-09-07T18:24:13.386Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T18:59:05.875Z/2022-04-04T18:59:05.875Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:43:39.620Z/2022-09-07T17:43:39.620Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:15:32.232Z/2022-09-07T18:15:32.232Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:49:31.370Z/2022-09-07T17:49:31.370Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:28:40.596Z/2022-04-04T19:28:40.596Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:16:01.302Z/2022-09-07T18:16:01.302Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:44:43.991Z/2022-09-07T17:44:43.991Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-04-04T19:21:41.445Z/2022-04-04T19:21:41.445Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T17:48:53.625Z/2022-09-07T17:48:53.625Z.mp4\n",
            "Copied: /content/dropbox_downloads2/2022-09-07T18:24:39.188Z/2022-09-07T18:24:39.188Z.mp4\n",
            "All .mp4 files are now in: /content/mp4_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZ_JGjbOxHQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O dropbox_files3.zip 'https://www.dropbox.com/scl/fo/6362bec5jklcio4jv0nyo/h?dl=1&rlkey=63a495mw7d5xw1k7kl4jwiea6&&dl=1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaFG302cxbv2",
        "outputId": "4ecff061-25fc-402b-8c23-272201f8b655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-20 18:11:51--  https://www.dropbox.com/scl/fo/6362bec5jklcio4jv0nyo/h?dl=1&rlkey=63a495mw7d5xw1k7kl4jwiea6&&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3f23ee286c3b5ad28dc9d7dc3b.dl.dropboxusercontent.com/zip_download_get/CCI9aktG7E5-9QBBfCFIRJilVwBTmAaPu6AWTXWkxaqAFjoCkFLJJWTuYp6GvCNf_GHr-LJUplTWOVzier4RnjyZomIqORlwdY3uwlfSn_ahUw# [following]\n",
            "--2024-11-20 18:11:53--  https://uc3f23ee286c3b5ad28dc9d7dc3b.dl.dropboxusercontent.com/zip_download_get/CCI9aktG7E5-9QBBfCFIRJilVwBTmAaPu6AWTXWkxaqAFjoCkFLJJWTuYp6GvCNf_GHr-LJUplTWOVzier4RnjyZomIqORlwdY3uwlfSn_ahUw\n",
            "Resolving uc3f23ee286c3b5ad28dc9d7dc3b.dl.dropboxusercontent.com (uc3f23ee286c3b5ad28dc9d7dc3b.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc3f23ee286c3b5ad28dc9d7dc3b.dl.dropboxusercontent.com (uc3f23ee286c3b5ad28dc9d7dc3b.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400165079 (382M) [application/zip]\n",
            "Saving to: ‘dropbox_files3.zip’\n",
            "\n",
            "dropbox_files3.zip  100%[===================>] 381.63M   103MB/s    in 3.7s    \n",
            "\n",
            "2024-11-20 18:11:57 (103 MB/s) - ‘dropbox_files3.zip’ saved [400165079/400165079]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(\"dropbox_files3.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dropbox_downloads3\")\n",
        "print(\"Files extracted to /content/dropbox_downloads3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd8ZPFw9xhTT",
        "outputId": "9834a947-faa0-4d5c-d093-96979e1375b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/dropbox_downloads3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = \"/content/dropbox_downloads3/\"\n",
        "mp4_output_dir = \"/content/mp4_files\"\n",
        "os.makedirs(mp4_output_dir, exist_ok=True)\n",
        "\n",
        "# Walk through the directory tree and copy all .mp4 files to the output directory\n",
        "for root, dirs, files in os.walk(source_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".mp4\"):\n",
        "            full_path = os.path.join(root, file)\n",
        "            shutil.copy(full_path, mp4_output_dir)\n",
        "            print(f\"Copied: {full_path}\")\n",
        "\n",
        "print(f\"All .mp4 files are now in: {mp4_output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y3VMQOZxl-_",
        "outputId": "5021887f-eb22-4e0a-a55e-b97d1ea6087a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied: /content/dropbox_downloads3/2022-04-04T18:41:15.478Z/2022-04-04T18:41:15.478Z.mp4\n",
            "Copied: /content/dropbox_downloads3/2022-04-04T19:28:57.167Z/2022-04-04T19:28:57.167Z.mp4\n",
            "Copied: /content/dropbox_downloads3/2022-04-04T19:07:55.583Z/2022-04-04T19:07:55.583Z.mp4\n",
            "Copied: /content/dropbox_downloads3/2022-04-04T18:58:12.589Z/2022-04-04T18:58:12.589Z.mp4\n",
            "Copied: /content/dropbox_downloads3/2022-04-04T18:59:05.875Z/2022-04-04T18:59:05.875Z.mp4\n",
            "Copied: /content/dropbox_downloads3/2022-04-04T19:21:41.445Z/2022-04-04T19:21:41.445Z.mp4\n",
            "All .mp4 files are now in: /content/mp4_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/Capstone - AI Guide Dog 2024'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0FmR5kZ5M4X",
        "outputId": "5264c6df-ffe3-4b18-b286-9451d99ab7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 11632_standup_quadchart_Nov21.pptx\t\t       'Guide Dog 2024 Master.gdoc'\n",
            " 11632_standup_quadchart_Nov7.pptx\t\t        Meeting_minutes.gdoc\n",
            " 11632_standup_quadchart_Oct10.pptx\t\t       'meeting recording'\n",
            " 11632_standup_quadchart_Sept26.pptx\t\t        output_frames\n",
            " 11634_ice_breaker_AI_Guide_Dog.gdoc\t\t        Rough_work.gdoc\n",
            "'2022 docs'\t\t\t\t\t        segmentation_masks\n",
            "'AI_Guide_Dog_2024_-_Capstone_Proposal_(2).pdf'         video1098796292.mp4\n",
            "'AI Guide Dog 2024 Midterm Presentation.gslides'        video1194195383.mp4\n",
            "'AI Guide Dog 2024 Spring Final Presentation.gslides'   video1753052460.mp4\n",
            "'AI Guide Dog Fall Plan 2024.pptx'\t\t        video2900402067.mp4\n",
            " AI_Guide_Dog_MidTerm_Presentation.pptx\t\t        videos\n",
            "'Baseline Dataset'\t\t\t\t       'Vision Document'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = '/content/mp4_files'  # Update with your source directory path\n",
        "destination_dir = '/content/drive/My Drive/Capstone - AI Guide Dog 2024/data_videos'  # Update with your target directory in Google Drive\n",
        "\n",
        "# Copy directory to Google Drive\n",
        "shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "print(f\"Directory copied to: {destination_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atFOoPJO5De4",
        "outputId": "d12749a6-a44d-41db-e9a0-7f6847123494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory copied to: /content/drive/My Drive/Capstone - AI Guide Dog 2024/data_videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get all frames"
      ],
      "metadata": {
        "id": "bipbYDPmxyc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Directory containing all .mp4 files\n",
        "mp4_dir = \"/content/mp4_files\"\n",
        "# Directory to save extracted frames\n",
        "frames_dir = \"/content/extracted_frames\"\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all .mp4 files\n",
        "for mp4_file in os.listdir(mp4_dir):\n",
        "    if mp4_file.endswith(\".mp4\"):\n",
        "        video_path = os.path.join(mp4_dir, mp4_file)\n",
        "        video_name = os.path.splitext(mp4_file)[0]  # Get video name without extension\n",
        "\n",
        "        # ffmpeg command to extract frames with unique filenames\n",
        "        cmd = [\n",
        "            \"ffmpeg\", \"-i\", video_path,\n",
        "            \"-vf\", \"fps=2,scale=480:270\",\n",
        "            f\"{frames_dir}/{video_name}_frame_%04d.png\"\n",
        "        ]\n",
        "\n",
        "        # Run the command\n",
        "        print(f\"Extracting frames for: {mp4_file}\")\n",
        "        subprocess.run(cmd)\n",
        "\n",
        "print(\"Frame extraction complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tItLzdccy3JG",
        "outputId": "9a032185-e0eb-4f00-8847-1187ba21e35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting frames for: 2022-04-04T18:48:45.704Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:28:40.596Z.mp4\n",
            "Extracting frames for: 2022-04-04T12:56:37.203Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:22:45.749Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:14:28.350Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:48:18.039Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:23:12.931Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:50:31.458Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:28:57.167Z.mp4\n",
            "Extracting frames for: 2022-04-04T12:51:41.445Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:47:02.517Z.mp4\n",
            "Extracting frames for: 2022-04-04T18:59:05.875Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:18:27.292Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:01:16.917Z.mp4\n",
            "Extracting frames for: 2022-04-04T13:15:19.655Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:09:43.644Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:24:10.208Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:28:21.899Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:15:13.682Z.mp4\n",
            "Extracting frames for: 2022-04-04T12:43:51.211Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:23:24.989Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:10:28.196Z.mp4\n",
            "Extracting frames for: 2022-04-04T18:41:15.478Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:21:40.015Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:19:26.012Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:06:29.642Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:16:45.412Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:23:39.867Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:02:28.038Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:11:48.827Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:18:52.214Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:43:39.620Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:17:19.405Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:10:36.407Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:21:41.445Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:24:13.386Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:04:56.768Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:15:43.550Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:19:33.984Z.mp4\n",
            "Extracting frames for: 2022-04-04T13:01:31.083Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:17:50.959Z.mp4\n",
            "Extracting frames for: 2022-04-04T18:58:12.589Z.mp4\n",
            "Extracting frames for: 2022-04-04T12:36:21.745Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:12:38.748Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:51:24.478Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:48:53.625Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:43:08.884Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:07:57.983Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:19:01.849Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:13:42.852Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:07:08.854Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:48:06.905Z.mp4\n",
            "Extracting frames for: 2022-04-04T12:14:43.949Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:16:01.302Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:20:59.483Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:50:02.923Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:15:32.232Z.mp4\n",
            "Extracting frames for: 2022-04-04T18:47:26.422Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:47:34.402Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:24:39.188Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:49:31.370Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:06:23.406Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:08:26.648Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:44:43.991Z.mp4\n",
            "Extracting frames for: 2022-04-04T19:07:55.583Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:16:48.983Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:43:16.918Z.mp4\n",
            "Extracting frames for: 2022-04-04T16:12:56.605Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:45:09.377Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:25:51.421Z.mp4\n",
            "Extracting frames for: 2022-09-07T18:21:23.281Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:42:05.175Z.mp4\n",
            "Extracting frames for: 2022-09-07T17:46:38.914Z.mp4\n",
            "Frame extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = '/content/extracted_frames'  # Update with your source directory path\n",
        "destination_dir = '/content/drive/My Drive/Capstone - AI Guide Dog 2024/extracted_frames'  # Update with your target directory in Google Drive\n",
        "\n",
        "# Copy directory to Google Drive\n",
        "shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "print(f\"Directory copied to: {destination_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3g7Ld2-5jPX",
        "outputId": "1b3f1efa-7280-4171-c005-d4cc5c3d56f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory copied to: /content/drive/My Drive/Capstone - AI Guide Dog 2024/extracted_frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process image and store segments"
      ],
      "metadata": {
        "id": "JQ6w26pRvQZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "def extract_and_save_segmentations(image_dir, segmentation_module, output_dir, batch_size=4):\n",
        "    \"\"\"\n",
        "    Extract segmentations for all images in a directory and save them.\n",
        "\n",
        "    Args:\n",
        "        image_dir: Directory containing input images\n",
        "        segmentation_module: Pretrained segmentation model\n",
        "        output_dir: Directory to save segmentation masks\n",
        "        batch_size: Batch size for processing\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Image transformation pipeline\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Get list of all images\n",
        "    image_files = [f for f in os.listdir(image_dir)]\n",
        "\n",
        "    for img_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        try:\n",
        "            # Load and process image\n",
        "            img_path = os.path.join(image_dir, img_file)\n",
        "            pil_image = PIL.Image.open(img_path).convert('RGB')\n",
        "            img_original = numpy.array(pil_image)\n",
        "            img_data = pil_to_tensor(pil_image)\n",
        "            singleton_batch = {'img_data': img_data[None].cuda()}\n",
        "            output_size = img_data.shape[1:]\n",
        "\n",
        "            # Generate segmentation\n",
        "            with torch.no_grad():\n",
        "                scores = segmentation_module(singleton_batch, segSize=output_size)\n",
        "\n",
        "            # Get prediction\n",
        "            _, pred = torch.max(scores, dim=1)\n",
        "            pred = pred.cpu()[0].numpy()\n",
        "            pred_tensor = torch.tensor(pred, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            # Save segmentation mask\n",
        "            output_path = os.path.join(output_dir, f\"{os.path.splitext(img_file)[0]}_segment.npy\")\n",
        "            np.save(output_path, pred)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_file}: {str(e)}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "3UE5AnORbYTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = 'extracted_frames'\n",
        "segmentation_output_dir = 'extracted_masks'\n",
        "\n",
        "# Load and normalize one image as a singleton tensor batch\n",
        "pil_to_tensor = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
        "        std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
        "])\n"
      ],
      "metadata": {
        "id": "QWHfVV9CcIUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### NO RERUN #######\n",
        "extract_and_save_segmentations(\n",
        "    image_dir=image_dir,\n",
        "    segmentation_module=segmentation_module,  # Your pretrained segmentation module\n",
        "    output_dir=segmentation_output_dir\n",
        ")"
      ],
      "metadata": {
        "id": "6IXohEbzdt_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2bd8a7e9-043a-4963-b630-6987154ac107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 7007/7007 [07:00<00:00, 16.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = '/content/extracted_masks'  # Update with your source directory path\n",
        "destination_dir = '/content/drive/My Drive/Capstone - AI Guide Dog 2024/extracted_masks'  # Update with your target directory in Google Drive\n",
        "\n",
        "# Copy directory to Google Drive\n",
        "shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "print(f\"Directory copied to: {destination_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxELXoej73cY",
        "outputId": "681cc046-18a0-4150-b6eb-328c6ac95130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory copied to: /content/drive/My Drive/Capstone - AI Guide Dog 2024/extracted_masks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_autoencoder(autoencoder, segmentation_dir, num_epochs=100, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Train autoencoder on stored segmentation masks, processing one image at a time.\n",
        "\n",
        "    Args:\n",
        "        autoencoder: Autoencoder model\n",
        "        segmentation_dir: Directory containing stored segmentation masks\n",
        "        num_epochs: Number of training epochs\n",
        "        learning_rate: Learning rate for optimizer\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    autoencoder = autoencoder.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Get list of all segmentation files\n",
        "    seg_files = [f for f in os.listdir(segmentation_dir)][:100]\n",
        "    print(seg_files)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        # Shuffle files at the start of each epoch\n",
        "        random.shuffle(seg_files)\n",
        "\n",
        "        for seg_file in tqdm(seg_files, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            try:\n",
        "                # Load single segmentation mask\n",
        "                seg_path = os.path.join(segmentation_dir, seg_file)\n",
        "                seg_mask = np.load(seg_path)\n",
        "                seg_tensor = torch.tensor(seg_mask, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                seg_tensor = seg_tensor.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                encoded, reconstructed = autoencoder(seg_tensor)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(reconstructed, seg_tensor)\n",
        "                print(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {seg_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / len(seg_files)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # Save checkpoint every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            checkpoint_path = f'autoencoder_checkpoint_epoch_{epoch+1}.pth'\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, checkpoint_path)\n",
        "\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "AVkfijrCXxVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 648  # Encoded dimension size\n",
        "autoencoder = MaskAutoencoder(height=270, width=480, k=k)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iH1ZOcqzZZac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_autoencoder = train_autoencoder(\n",
        "        autoencoder=autoencoder,\n",
        "        segmentation_dir=segmentation_output_dir,\n",
        "        num_epochs=1\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1n4Krfyc6BZ",
        "outputId": "480eaf8f-916f-48e0-ebf3-5ba7d8dc33de",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['frame_0001_seg.npy', 'frame_0002_seg.npy', 'frame_0003_seg.npy', 'frame_0004_seg.npy', 'frame_0005_seg.npy', 'frame_0006_seg.npy', 'frame_0007_seg.npy', 'frame_0008_seg.npy', 'frame_0009_seg.npy', 'frame_0010_seg.npy', 'frame_0011_seg.npy', 'frame_0012_seg.npy', 'frame_0013_seg.npy', 'frame_0014_seg.npy', 'frame_0015_seg.npy', 'frame_0016_seg.npy', 'frame_0017_seg.npy', 'frame_0018_seg.npy', 'frame_0019_seg.npy', 'frame_0020_seg.npy', 'frame_0021_seg.npy', 'frame_0022_seg.npy', 'frame_0023_seg.npy', 'frame_0024_seg.npy', 'frame_0025_seg.npy', 'frame_0026_seg.npy', 'frame_0027_seg.npy', 'frame_0028_seg.npy', 'frame_0029_seg.npy', 'frame_0030_seg.npy', 'frame_0031_seg.npy', 'frame_0032_seg.npy', 'frame_0033_seg.npy', 'frame_0034_seg.npy', 'frame_0035_seg.npy', 'frame_0036_seg.npy', 'frame_0037_seg.npy', 'frame_0038_seg.npy', 'frame_0039_seg.npy', 'frame_0040_seg.npy', 'frame_0041_seg.npy', 'frame_0042_seg.npy', 'frame_0043_seg.npy', 'frame_0044_seg.npy', 'frame_0045_seg.npy', 'frame_0046_seg.npy', 'frame_0047_seg.npy', 'frame_0048_seg.npy', 'frame_0049_seg.npy', 'frame_0050_seg.npy', 'frame_0051_seg.npy', 'frame_0052_seg.npy', 'frame_0053_seg.npy', 'frame_0054_seg.npy', 'frame_0055_seg.npy', 'frame_0056_seg.npy', 'frame_0057_seg.npy', 'frame_0058_seg.npy', 'frame_0059_seg.npy', 'frame_0060_seg.npy', 'frame_0061_seg.npy', 'frame_0062_seg.npy', 'frame_0063_seg.npy', 'frame_0064_seg.npy', 'frame_0065_seg.npy', 'frame_0066_seg.npy', 'frame_0067_seg.npy', 'frame_0068_seg.npy', 'frame_0069_seg.npy', 'frame_0070_seg.npy', 'frame_0071_seg.npy', 'frame_0072_seg.npy', 'frame_0073_seg.npy', 'frame_0074_seg.npy', 'frame_0075_seg.npy', 'frame_0076_seg.npy', 'frame_0077_seg.npy', 'frame_0078_seg.npy', 'frame_0079_seg.npy', 'frame_0080_seg.npy', 'frame_0081_seg.npy', 'frame_0082_seg.npy', 'frame_0083_seg.npy', 'frame_0084_seg.npy', 'frame_0085_seg.npy', 'frame_0086_seg.npy', 'frame_0087_seg.npy', 'frame_0088_seg.npy', 'frame_0089_seg.npy', 'frame_0090_seg.npy', 'frame_0091_seg.npy', 'frame_0092_seg.npy', 'frame_0093_seg.npy', 'frame_0094_seg.npy', 'frame_0095_seg.npy', 'frame_0096_seg.npy', 'frame_0097_seg.npy', 'frame_0098_seg.npy', 'frame_0099_seg.npy', 'frame_0100_seg.npy']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   1%|          | 1/100 [00:00<01:06,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(96.4983, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0038_seg.npy: CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 14.75 GiB of which 537.06 MiB is free. Process 22363 has 14.22 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:   2%|▏         | 2/100 [00:01<00:53,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(837.6417, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0004_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:   3%|▎         | 3/100 [00:01<00:49,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(497.2705, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0048_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   7%|▋         | 7/100 [00:04<00:48,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(313.9653, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0073_seg.npy: 'exp_avg'\n",
            "tensor(916.1234, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0066_seg.npy: 'exp_avg'\n",
            "tensor(812.8948, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0011_seg.npy: 'exp_avg'\n",
            "tensor(402.5553, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0049_seg.npy: 'exp_avg'\n",
            "tensor(665.7058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0084_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  13%|█▎        | 13/100 [00:04<00:16,  5.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(643.3438, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0078_seg.npy: 'exp_avg'\n",
            "tensor(1321.9193, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0096_seg.npy: 'exp_avg'\n",
            "tensor(502.3122, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0085_seg.npy: 'exp_avg'\n",
            "tensor(226.1879, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0064_seg.npy: 'exp_avg'\n",
            "tensor(856.9815, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0002_seg.npy: 'exp_avg'\n",
            "tensor(292.3311, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0032_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  19%|█▉        | 19/100 [00:04<00:08,  9.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(764.1118, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0009_seg.npy: 'exp_avg'\n",
            "tensor(625.5708, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0056_seg.npy: 'exp_avg'\n",
            "tensor(337.4893, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0029_seg.npy: 'exp_avg'\n",
            "tensor(622.4697, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0035_seg.npy: 'exp_avg'\n",
            "tensor(978.7559, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0069_seg.npy: 'exp_avg'\n",
            "tensor(695.9456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0001_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  25%|██▌       | 25/100 [00:05<00:05, 14.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1065.7792, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0008_seg.npy: 'exp_avg'\n",
            "tensor(585.3024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0059_seg.npy: 'exp_avg'\n",
            "tensor(618.0959, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0058_seg.npy: 'exp_avg'\n",
            "tensor(1385.3262, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0097_seg.npy: 'exp_avg'\n",
            "tensor(950.8318, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0067_seg.npy: 'exp_avg'\n",
            "tensor(654.2276, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0046_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  31%|███       | 31/100 [00:05<00:03, 19.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(979.1251, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0045_seg.npy: 'exp_avg'\n",
            "tensor(141.0316, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0021_seg.npy: 'exp_avg'\n",
            "tensor(2026.2252, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0044_seg.npy: 'exp_avg'\n",
            "tensor(380.2632, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0063_seg.npy: 'exp_avg'\n",
            "tensor(1043.5345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0012_seg.npy: 'exp_avg'\n",
            "tensor(928.2049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0071_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  37%|███▋      | 37/100 [00:05<00:02, 21.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(324.3942, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0026_seg.npy: 'exp_avg'\n",
            "tensor(462.0553, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0040_seg.npy: 'exp_avg'\n",
            "tensor(29.3434, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0019_seg.npy: 'exp_avg'\n",
            "tensor(954.8893, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0089_seg.npy: 'exp_avg'\n",
            "tensor(659.8142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0041_seg.npy: 'exp_avg'\n",
            "tensor(628.8503, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0006_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:  40%|████      | 40/100 [00:05<00:02, 23.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(475.7962, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0037_seg.npy: 'exp_avg'\n",
            "tensor(13.1306, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0023_seg.npy: 'exp_avg'\n",
            "tensor(708.4498, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0082_seg.npy: 'exp_avg'\n",
            "tensor(515.8945, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0061_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  46%|████▌     | 46/100 [00:05<00:02, 21.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1396.1909, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0100_seg.npy: 'exp_avg'\n",
            "tensor(946.5985, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0050_seg.npy: 'exp_avg'\n",
            "tensor(470.6251, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0030_seg.npy: 'exp_avg'\n",
            "tensor(732.0334, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0014_seg.npy: 'exp_avg'\n",
            "tensor(1337.0741, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0094_seg.npy: 'exp_avg'\n",
            "tensor(457.2302, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0057_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  52%|█████▏    | 52/100 [00:06<00:02, 23.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(781.8328, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0017_seg.npy: 'exp_avg'\n",
            "tensor(559.9414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0077_seg.npy: 'exp_avg'\n",
            "tensor(1033.9462, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0095_seg.npy: 'exp_avg'\n",
            "tensor(1142.7010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0007_seg.npy: 'exp_avg'\n",
            "tensor(1145.8796, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0081_seg.npy: 'exp_avg'\n",
            "tensor(1232.7216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0098_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  58%|█████▊    | 58/100 [00:06<00:01, 24.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1431.9952, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0099_seg.npy: 'exp_avg'\n",
            "tensor(181.0830, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0091_seg.npy: 'exp_avg'\n",
            "tensor(654.1749, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0079_seg.npy: 'exp_avg'\n",
            "tensor(97.1413, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0020_seg.npy: 'exp_avg'\n",
            "tensor(244.4093, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0068_seg.npy: 'exp_avg'\n",
            "tensor(1171.9071, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0055_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  64%|██████▍   | 64/100 [00:06<00:01, 25.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(853.0808, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0051_seg.npy: 'exp_avg'\n",
            "tensor(98.0797, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0043_seg.npy: 'exp_avg'\n",
            "tensor(1562.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0053_seg.npy: 'exp_avg'\n",
            "tensor(724.5674, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0088_seg.npy: 'exp_avg'\n",
            "tensor(1343.9102, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0093_seg.npy: 'exp_avg'\n",
            "tensor(903.4429, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0003_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  70%|███████   | 70/100 [00:06<00:01, 24.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(406.9242, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0042_seg.npy: 'exp_avg'\n",
            "tensor(850.2705, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0072_seg.npy: 'exp_avg'\n",
            "tensor(491.4728, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0047_seg.npy: 'exp_avg'\n",
            "tensor(647.0845, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0087_seg.npy: 'exp_avg'\n",
            "tensor(402.3696, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0039_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  76%|███████▌  | 76/100 [00:07<00:00, 25.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(368.8496, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0065_seg.npy: 'exp_avg'\n",
            "tensor(264.5784, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0022_seg.npy: 'exp_avg'\n",
            "tensor(557.2831, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0062_seg.npy: 'exp_avg'\n",
            "tensor(882.3905, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0054_seg.npy: 'exp_avg'\n",
            "tensor(1058.0135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0015_seg.npy: 'exp_avg'\n",
            "tensor(312.7843, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0025_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  82%|████████▏ | 82/100 [00:07<00:00, 25.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(553.5720, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0075_seg.npy: 'exp_avg'\n",
            "tensor(382.7483, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0024_seg.npy: 'exp_avg'\n",
            "tensor(466.4609, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0074_seg.npy: 'exp_avg'\n",
            "tensor(1044.8114, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0080_seg.npy: 'exp_avg'\n",
            "tensor(625.2137, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0036_seg.npy: 'exp_avg'\n",
            "tensor(1411.7313, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0052_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  88%|████████▊ | 88/100 [00:07<00:00, 20.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(629.7054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0005_seg.npy: 'exp_avg'\n",
            "tensor(392.1633, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0018_seg.npy: 'exp_avg'\n",
            "tensor(862.4950, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0070_seg.npy: 'exp_avg'\n",
            "tensor(757.5562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0013_seg.npy: 'exp_avg'\n",
            "tensor(930.7218, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0016_seg.npy: 'exp_avg'\n",
            "tensor(739.2895, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0086_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  94%|█████████▍| 94/100 [00:07<00:00, 22.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(584.7433, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0031_seg.npy: 'exp_avg'\n",
            "tensor(316.5999, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0027_seg.npy: 'exp_avg'\n",
            "tensor(349.2674, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0033_seg.npy: 'exp_avg'\n",
            "tensor(439.7694, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0076_seg.npy: 'exp_avg'\n",
            "tensor(272.4155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0090_seg.npy: 'exp_avg'\n",
            "tensor(236.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0092_seg.npy: 'exp_avg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 100/100 [00:08<00:00, 12.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(996.3928, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0010_seg.npy: 'exp_avg'\n",
            "tensor(321.0468, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0083_seg.npy: 'exp_avg'\n",
            "tensor(322.7357, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0060_seg.npy: 'exp_avg'\n",
            "tensor(604.5424, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0034_seg.npy: 'exp_avg'\n",
            "tensor(640.1530, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "Error processing frame_0028_seg.npy: 'exp_avg'\n",
            "Epoch 1, Average Loss: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(trained_autoencoder.state_dict(), 'trained_autoencoder_final.pth')"
      ],
      "metadata": {
        "id": "RymPsyokaa5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with dataloader for efficiency\n",
        "def train_autoencoder(autoencoder, segmentation_dir, num_epochs=100, learning_rate=0.001, batch_size=16):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    autoencoder = autoencoder.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Create a DataLoader with batching\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            seg_tensor = batch.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            encoded, reconstructed = autoencoder(seg_tensor)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(reconstructed, seg_tensor)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            checkpoint_path = f'autoencoder_checkpoint_epoch_{epoch+1}.pth'\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, checkpoint_path)\n",
        "    return autoencoder\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, segmentation_dir, file_list):\n",
        "        self.segmentation_dir = segmentation_dir\n",
        "        self.file_list = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seg_file = self.file_list[idx]\n",
        "        seg_path = os.path.join(self.segmentation_dir, seg_file)\n",
        "        seg_mask = np.load(seg_path)\n",
        "        return torch.tensor(seg_mask, dtype=torch.float32).unsqueeze(0)"
      ],
      "metadata": {
        "id": "u5VCzMwf85Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 648  # Encoded dimension size\n",
        "autoencoder = MaskAutoencoder(height=270, width=480, k=k)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fQuDZjP_9jz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with train-test split\n",
        "def train_autoencoder(autoencoder, segmentation_dir, num_epochs=100, learning_rate=0.001, batch_size=16, eval_frequency=10):\n",
        "    # Create dataset\n",
        "    seg_files = [f for f in os.listdir(segmentation_dir)]\n",
        "    full_dataset = SegmentationDataset(segmentation_dir, seg_files)\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    autoencoder = autoencoder.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop with test evaluation\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        autoencoder.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            seg_tensor = batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            encoded, reconstructed = autoencoder(seg_tensor)\n",
        "            loss = criterion(reconstructed, seg_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        print(f\"Train Loss: {train_loss/len(train_loader):.6f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        if (epoch + 1) % eval_frequency == 0:\n",
        "          autoencoder.eval()\n",
        "          val_loss = 0\n",
        "          with torch.no_grad():\n",
        "              for batch in test_loader:\n",
        "                  seg_tensor = batch.to(device)\n",
        "                  encoded, reconstructed = autoencoder(seg_tensor)\n",
        "                  loss = criterion(reconstructed, seg_tensor)\n",
        "                  val_loss += loss.item()\n",
        "              print(f\"Validation Loss: {val_loss/len(test_loader):.6f}\")\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            checkpoint_path = f'autoencoder_checkpoint_epoch_{epoch+1}.pth'\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, checkpoint_path)\n",
        "\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "mmeb1VW9Ag3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_autoencoder = train_autoencoder(\n",
        "        autoencoder=autoencoder,\n",
        "        segmentation_dir='extracted_masks',\n",
        "        num_epochs=50\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13e5837-36c0-4de6-ad94-7cefc3343a71",
        "collapsed": true,
        "id": "_AD47o929jz3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50: 100%|██████████| 438/438 [02:39<00:00,  2.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 653.636256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/50: 100%|██████████| 438/438 [02:31<00:00,  2.88it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Average Loss: 653.623959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/50: 100%|██████████| 438/438 [02:26<00:00,  3.00it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Average Loss: 653.546840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/50: 100%|██████████| 438/438 [02:25<00:00,  3.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Average Loss: 653.611095\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/50: 100%|██████████| 438/438 [02:23<00:00,  3.04it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Average Loss: 653.597986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 438/438 [02:21<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Loss: 653.536078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 438/438 [02:20<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Loss: 653.518660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 438/438 [02:16<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Loss: 653.565742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 438/438 [02:14<00:00,  3.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Loss: 653.552168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 438/438 [02:10<00:00,  3.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Loss: 653.536049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Average Loss: 653.536043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Average Loss: 653.529989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Average Loss: 653.529931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Average Loss: 653.574474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Average Loss: 653.567729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 438/438 [02:05<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Average Loss: 653.557613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Average Loss: 653.540401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 438/438 [02:05<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Average Loss: 653.526081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Average Loss: 653.520010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Average Loss: 653.569354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21, Average Loss: 653.597220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22, Average Loss: 653.620864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23, Average Loss: 653.535454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24, Average Loss: 653.533524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25, Average Loss: 653.540995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26, Average Loss: 653.543492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27, Average Loss: 653.531279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28, Average Loss: 653.555228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29, Average Loss: 653.507739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Average Loss: 653.511117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31, Average Loss: 653.585588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32, Average Loss: 653.497608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33, Average Loss: 653.513476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34, Average Loss: 653.532347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35, Average Loss: 653.567354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36, Average Loss: 653.548174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37, Average Loss: 653.590653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38, Average Loss: 653.529242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39, Average Loss: 653.551089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Average Loss: 653.519890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41, Average Loss: 653.542692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42, Average Loss: 653.502513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43, Average Loss: 653.528728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44, Average Loss: 653.546500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45, Average Loss: 653.498348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46, Average Loss: 653.560366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47, Average Loss: 653.558996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48, Average Loss: 653.522894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49, Average Loss: 653.558751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Average Loss: 653.513968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = 'content'  # Update with your directory path\n",
        "zip_file = '/content/directory_name.zip'  # Update with your desired zip file name\n",
        "\n",
        "# Compress the directory into a zip file\n",
        "shutil.make_archive(zip_file.replace('.zip', ''), 'zip', source_dir)\n",
        "\n",
        "print(f\"Directory compressed to {zip_file}\")"
      ],
      "metadata": {
        "id": "57Gy-n0uXKqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epoch 1/50: 100%|██████████| 438/438 [02:39<00:00,  2.74it/s]\n",
        "# Epoch 1, Average Loss: 653.636256\n",
        "# Epoch 2/50: 100%|██████████| 438/438 [02:31<00:00,  2.88it/s]\n",
        "# Epoch 2, Average Loss: 653.623959\n",
        "# Epoch 3/50: 100%|██████████| 438/438 [02:26<00:00,  3.00it/s]\n",
        "# Epoch 3, Average Loss: 653.546840\n",
        "# Epoch 4/50: 100%|██████████| 438/438 [02:25<00:00,  3.02it/s]\n",
        "# Epoch 4, Average Loss: 653.611095\n",
        "# Epoch 5/50: 100%|██████████| 438/438 [02:23<00:00,  3.04it/s]\n",
        "# Epoch 5, Average Loss: 653.597986\n",
        "# Epoch 6/50: 100%|██████████| 438/438 [02:21<00:00,  3.10it/s]\n",
        "# Epoch 6, Average Loss: 653.536078\n",
        "# Epoch 7/50: 100%|██████████| 438/438 [02:20<00:00,  3.12it/s]\n",
        "# Epoch 7, Average Loss: 653.518660\n",
        "# Epoch 8/50: 100%|██████████| 438/438 [02:16<00:00,  3.20it/s]\n",
        "# Epoch 8, Average Loss: 653.565742\n",
        "# Epoch 9/50: 100%|██████████| 438/438 [02:14<00:00,  3.26it/s]\n",
        "# Epoch 9, Average Loss: 653.552168\n",
        "# Epoch 10/50: 100%|██████████| 438/438 [02:10<00:00,  3.36it/s]\n",
        "# Epoch 10, Average Loss: 653.536049\n",
        "# Epoch 11/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 11, Average Loss: 653.536043\n",
        "# Epoch 12/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 12, Average Loss: 653.529989\n",
        "# Epoch 13/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 13, Average Loss: 653.529931\n",
        "# Epoch 14/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 14, Average Loss: 653.574474\n",
        "# Epoch 15/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 15, Average Loss: 653.567729\n",
        "# Epoch 16/50: 100%|██████████| 438/438 [02:05<00:00,  3.50it/s]\n",
        "# Epoch 16, Average Loss: 653.557613\n",
        "# Epoch 17/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 17, Average Loss: 653.540401\n",
        "# Epoch 18/50: 100%|██████████| 438/438 [02:05<00:00,  3.50it/s]\n",
        "# Epoch 18, Average Loss: 653.526081\n",
        "# Epoch 19/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 19, Average Loss: 653.520010\n",
        "# Epoch 20/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 20, Average Loss: 653.569354\n",
        "# Epoch 21/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 21, Average Loss: 653.597220\n",
        "# Epoch 22/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 22, Average Loss: 653.620864\n",
        "# Epoch 23/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 23, Average Loss: 653.535454\n",
        "# Epoch 24/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 24, Average Loss: 653.533524\n",
        "# Epoch 25/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 25, Average Loss: 653.540995\n",
        "# Epoch 26/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 26, Average Loss: 653.543492\n",
        "# Epoch 27/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 27, Average Loss: 653.531279\n",
        "# Epoch 28/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 28, Average Loss: 653.555228\n",
        "# Epoch 29/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 29, Average Loss: 653.507739\n",
        "# Epoch 30/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 30, Average Loss: 653.511117\n",
        "# Epoch 31/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 31, Average Loss: 653.585588\n",
        "# Epoch 32/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 32, Average Loss: 653.497608\n",
        "# Epoch 33/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 33, Average Loss: 653.513476\n",
        "# Epoch 34/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 34, Average Loss: 653.532347\n",
        "# Epoch 35/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 35, Average Loss: 653.567354\n",
        "# Epoch 36/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 36, Average Loss: 653.548174\n",
        "# Epoch 37/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 37, Average Loss: 653.590653\n",
        "# Epoch 38/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 38, Average Loss: 653.529242\n",
        "# Epoch 39/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 39, Average Loss: 653.551089\n",
        "# Epoch 40/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 40, Average Loss: 653.519890\n",
        "# Epoch 41/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 41, Average Loss: 653.542692\n",
        "# Epoch 42/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 42, Average Loss: 653.502513\n",
        "# Epoch 43/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 43, Average Loss: 653.528728\n",
        "# Epoch 44/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 44, Average Loss: 653.546500\n",
        "# Epoch 45/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 45, Average Loss: 653.498348\n",
        "# Epoch 46/50: 100%|██████████| 438/438 [02:05<00:00,  3.48it/s]\n",
        "# Epoch 46, Average Loss: 653.560366\n",
        "# Epoch 47/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 47, Average Loss: 653.558996\n",
        "# Epoch 48/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 48, Average Loss: 653.522894\n",
        "# Epoch 49/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]\n",
        "# Epoch 49, Average Loss: 653.558751\n",
        "# Epoch 50/50: 100%|██████████| 438/438 [02:05<00:00,  3.49it/s]Epoch 50, Average Loss: 653.513968"
      ],
      "metadata": {
        "id": "SiqrKC2t-YFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
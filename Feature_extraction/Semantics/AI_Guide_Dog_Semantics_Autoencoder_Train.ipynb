{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG_YUKHCRLbw"
   },
   "source": [
    "# Semantic Segmentation Demo\n",
    "\n",
    "This is a notebook for running the benchmark semantic segmentation network from the the [ADE20K MIT Scene Parsing Benchchmark](http://sceneparsing.csail.mit.edu/).\n",
    "\n",
    "The code for this notebook is available here\n",
    "https://github.com/CSAILVision/semantic-segmentation-pytorch/tree/master/notebooks\n",
    "\n",
    "It can be run on Colab at this URL https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1WI9ZhkRLb0"
   },
   "source": [
    "### Environment Setup\n",
    "\n",
    "First, download the code and pretrained models if we are on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKNrL-7iRLb1",
    "outputId": "d209be90-188a-4272-f9c2-de207df4fc47"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# # Colab-specific setup\n",
    "# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "# pip install yacs 2>&1 >> install.log\n",
    "# git init 2>&1 >> install.log\n",
    "# git remote add origin https://github.com/CSAILVision/semantic-segmentation-pytorch.git 2>> install.log\n",
    "# git pull origin master 2>&1 >> install.log\n",
    "# DOWNLOAD_ONLY=1 ./demo_test.sh 2>> install.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)  # Should show the installed PyTorch version\n",
    "# print(torch.cuda.is_available())  # Should return True if CUDA is working\n",
    "# print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/CSAILVision/semantic-segmentation-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd semantic-segmentation-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./semantic-segmentation-pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls semantic-segmentation-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLW4DaamRLb3"
   },
   "source": [
    "## Imports and utility functions\n",
    "\n",
    "We need pytorch, numpy, and the code for the segmentation model.  And some utilities for visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUVvmZObRLb4"
   },
   "outputs": [],
   "source": [
    "# System libs\n",
    "import os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\n",
    "# Our libs\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.utils import colorEncode\n",
    "\n",
    "# colors = scipy.io.loadmat('data/color150.mat')['colors']\n",
    "# names = {}\n",
    "\n",
    "# with open('data/object150_info.csv') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     next(reader)\n",
    "#     for row in reader:\n",
    "#         names[int(row[0])] = row[5].split(\";\")[0]\n",
    "\n",
    "# def visualize_result(img, pred, index=None):\n",
    "#     # filter prediction class if requested\n",
    "#     if index is not None:\n",
    "#         pred = pred.copy()\n",
    "#         pred[pred != index] = -1\n",
    "#         print(f'{names[index+1]}:')\n",
    "\n",
    "#     # colorize prediction\n",
    "#     pred_color = colorEncode(pred, colors).astype(numpy.uint8)\n",
    "\n",
    "#     # aggregate images and save\n",
    "#     im_vis = numpy.concatenate((img, pred_color), axis=1)\n",
    "#     display(PIL.Image.fromarray(im_vis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkdir -p ckpt/ade20k-resnet50dilated-ppm_deepsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import urllib.request\n",
    "\n",
    "# # Create directory if it doesn't exist\n",
    "# ckpt_dir = 'ckpt/ade20k-resnet50dilated-ppm_deepsup'\n",
    "# os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# # Download encoder weights\n",
    "# encoder_url = \"http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth\"\n",
    "# encoder_path = f\"{ckpt_dir}/encoder_epoch_20.pth\"\n",
    "# urllib.request.urlretrieve(encoder_url, encoder_path)\n",
    "\n",
    "# # Download decoder weights\n",
    "# decoder_url = \"http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth\"\n",
    "# decoder_path = f\"{ckpt_dir}/decoder_epoch_20.pth\"\n",
    "# urllib.request.urlretrieve(decoder_url, decoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceq0V_ifRLb5"
   },
   "source": [
    "## Loading the segmentation model\n",
    "\n",
    "Here we load a pretrained segmentation model.  Like any pytorch model, we can call it like a function, or examine the parameters in all the layers.\n",
    "\n",
    "After loading, we put it on the GPU.  And since we are doing inference, not training, we put the model in eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6nEb5y-RLb6",
    "outputId": "a5eb0399-1759-495a-f631-142da9fbb182"
   },
   "outputs": [],
   "source": [
    "# Network Builders\n",
    "net_encoder = ModelBuilder.build_encoder(\n",
    "    arch='resnet50dilated',\n",
    "    fc_dim=2048,\n",
    "    weights='ckpt/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth')\n",
    "net_decoder = ModelBuilder.build_decoder(\n",
    "    arch='ppm_deepsup',\n",
    "    fc_dim=2048,\n",
    "    num_class=150,\n",
    "    weights='ckpt/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth',\n",
    "    use_softmax=True)\n",
    "\n",
    "crit = torch.nn.NLLLoss(ignore_index=-1)\n",
    "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "segmentation_module.eval()\n",
    "segmentation_module.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UhjERUcRLb6"
   },
   "source": [
    "## Segmentation Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtSD9pRYRLb7"
   },
   "outputs": [],
   "source": [
    "# Load and normalize one image as a singleton tensor batch\n",
    "pil_to_tensor = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
    "        std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QK-Hrv-4RLb7"
   },
   "source": [
    "## Run the Model\n",
    "\n",
    "Finally we just pass the test image to the segmentation model.\n",
    "\n",
    "The segmentation model is coded as a function that takes a dictionary as input, because it wants to know both the input batch image data as well as the desired output segmentation resolution.  We ask for full resolution output.\n",
    "\n",
    "Then we use the previously-defined visualize_result function to render the segmentation map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX4sThIpRLb9"
   },
   "source": [
    "## Showing classes individually\n",
    "\n",
    "To see which colors are which, here we visualize individual classes, one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1x54BXzRLb9"
   },
   "outputs": [],
   "source": [
    "# # Top classes in answer\n",
    "# predicted_classes = numpy.bincount(pred.flatten()).argsort()[::-1]\n",
    "# for c in predicted_classes[:15]:\n",
    "#     visualize_result(img_original, pred, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t00lVh8IVOuZ",
    "outputId": "e3b42613-d992-496c-8ecc-e2014b9d4813"
   },
   "outputs": [],
   "source": [
    "# pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXPFX2SkXFsd"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_R0lHq7qXE-z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MaskAutoencoder(nn.Module):\n",
    "    def __init__(self, height, width, k):\n",
    "        super(MaskAutoencoder, self).__init__()\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Calculating dimensions after downsampling\n",
    "        def conv_output_size(input_size, kernel_size=3, stride=2, padding=1):\n",
    "            return (input_size + 2*padding - kernel_size) // stride + 1\n",
    "\n",
    "        h_after_conv1 = conv_output_size(height)\n",
    "        w_after_conv1 = conv_output_size(width)\n",
    "        h_after_conv2 = conv_output_size(h_after_conv1)\n",
    "        w_after_conv2 = conv_output_size(w_after_conv1)\n",
    "\n",
    "        self.encoded_dim = h_after_conv2 * w_after_conv2 * 64\n",
    "        #self.encoded_dim = (height // 4) * (width // 4) * 64\n",
    "        self.projection = nn.Linear(self.encoded_dim, k)\n",
    "\n",
    "        # Decoder components\n",
    "        self.unprojection = nn.Linear(k, self.encoded_dim)\n",
    "        self.unflatten = nn.Unflatten(1, (64, h_after_conv2, w_after_conv2))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            #nn.PixelShuffle(1), # Not fix input and reconstructed size mismtach\n",
    "            nn.Sigmoid()  # Sigmoid activation for pixel values in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        flattened = self.flatten(features)\n",
    "\n",
    "        projected = self.projection(flattened)\n",
    "\n",
    "        # Decoding\n",
    "        unprojected = self.unprojection(projected)\n",
    "\n",
    "        unflattened = self.unflatten(unprojected)\n",
    "\n",
    "        reconstructed = self.decoder(unflattened)\n",
    "        reconstructed = reconstructed[:, :, :x.shape[2], :x.shape[3]] # align output size with input size\n",
    "\n",
    "        return projected, reconstructed\n",
    "\n",
    "# Example reconstruction loss calculation\n",
    "def reconstruction_loss(original, reconstructed):\n",
    "    loss_fn = nn.MSELoss()  # Mean Squared Error for pixel-wise comparison\n",
    "    return loss_fn(reconstructed, original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UE5AnORbYTX"
   },
   "outputs": [],
   "source": [
    "# pil_to_tensor = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(\n",
    "#         mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
    "#         std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
    "# ])\n",
    "\n",
    "# def extract_and_save_segmentations(image_dir, segmentation_module, output_dir, batch_size=4):\n",
    "#     \"\"\"\n",
    "#     Extract segmentations for all images in a directory and save them.\n",
    "\n",
    "#     Args:\n",
    "#         image_dir: Directory containing input images\n",
    "#         segmentation_module: Pretrained segmentation model\n",
    "#         output_dir: Directory to save segmentation masks\n",
    "#         batch_size: Batch size for processing\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Image transformation pipeline\n",
    "#     transform = torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToTensor(),\n",
    "#         torchvision.transforms.Normalize(\n",
    "#             mean=[0.485, 0.456, 0.406],\n",
    "#             std=[0.229, 0.224, 0.225]\n",
    "#         )\n",
    "#     ])\n",
    "\n",
    "#     # Get list of all images\n",
    "#     image_files = [f for f in os.listdir(image_dir)]\n",
    "\n",
    "#     for img_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "#         try:\n",
    "#             # Load and process image\n",
    "#             img_path = os.path.join(image_dir, img_file)\n",
    "#             pil_image = PIL.Image.open(img_path).convert('RGB')\n",
    "#             img_original = numpy.array(pil_image)\n",
    "#             img_data = pil_to_tensor(pil_image)\n",
    "#             singleton_batch = {'img_data': img_data[None].cuda()}\n",
    "#             output_size = img_data.shape[1:]\n",
    "\n",
    "#             # Generate segmentation\n",
    "#             with torch.no_grad():\n",
    "#                 scores = segmentation_module(singleton_batch, segSize=output_size)\n",
    "\n",
    "#             # Get prediction\n",
    "#             _, pred = torch.max(scores, dim=1)\n",
    "#             pred = pred.cpu()[0].numpy()\n",
    "#             pred_tensor = torch.tensor(pred, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "#             # Save segmentation mask\n",
    "#             output_path = os.path.join(output_dir, f\"{os.path.splitext(img_file)[0]}_seg.npy\")\n",
    "#             np.save(output_path, pred)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {img_file}: {str(e)}\")\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWHfVV9CcIUP"
   },
   "outputs": [],
   "source": [
    "# # Colab\n",
    "# image_dir = '/content/drive/My Drive/Capstone - AI Guide Dog 2024/output_frames'\n",
    "# segmentation_output_dir = '/content/drive/My Drive/Capstone - AI Guide Dog 2024/segmentation_masks'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_output_dir = 'segmentation_masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IXohEbzdt_0"
   },
   "outputs": [],
   "source": [
    "# #### NO RERUN #######\n",
    "# inp = 'output_frames'\n",
    "# out = 'output2_masks/'\n",
    "# extract_and_save_segmentations(\n",
    "#     image_dir=inp,\n",
    "#     segmentation_module=segmentation_module,  # Your pretrained segmentation module\n",
    "#     output_dir=out\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, segmentation_dir, file_list):\n",
    "        self.segmentation_dir = segmentation_dir\n",
    "        self.file_list = file_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seg_file = self.file_list[idx]\n",
    "        seg_path = os.path.join(self.segmentation_dir, seg_file)\n",
    "        seg_mask = np.load(seg_path)\n",
    "        return torch.tensor(seg_mask, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVkfijrCXxVe"
   },
   "outputs": [],
   "source": [
    "# with train-test split\n",
    "def train_autoencoder(autoencoder, segmentation_dir, num_epochs=100, learning_rate=0.001, batch_size=16, eval_frequency=10):\n",
    "    # Create dataset\n",
    "    seg_files = [f for f in os.listdir(segmentation_dir)]\n",
    "    full_dataset = SegmentationDataset(segmentation_dir, seg_files)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    autoencoder = autoencoder.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop with test evaluation\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        autoencoder.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            seg_tensor = batch.to(device)\n",
    "            seg_tensor = seg_tensor.float() / 255.0\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            encoded, reconstructed = autoencoder(seg_tensor)\n",
    "            loss = criterion(reconstructed, seg_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.6f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        if (epoch + 1) % eval_frequency == 0:\n",
    "          autoencoder.eval()\n",
    "          val_loss = 0\n",
    "          with torch.no_grad():\n",
    "              for batch in test_loader:\n",
    "                  seg_tensor = batch.to(device)\n",
    "                  encoded, reconstructed = autoencoder(seg_tensor)\n",
    "                  loss = criterion(reconstructed, seg_tensor)\n",
    "                  val_loss += loss.item()\n",
    "              print(f\"Validation Loss: {val_loss/len(test_loader):.6f}\")\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = f'autoencoder_checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': autoencoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH1ZOcqzZZac"
   },
   "outputs": [],
   "source": [
    "k = 648  # Encoded dimension size\n",
    "autoencoder = MaskAutoencoder(height=270, width=480, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CyclicLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1n4Krfyc6BZ",
    "outputId": "480eaf8f-916f-48e0-ebf3-5ba7d8dc33de"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "segmentation_dir = 'segmentation_input/extracted_masks'\n",
    "seg_files = [f for f in os.listdir(segmentation_dir)]\n",
    "full_dataset = SegmentationDataset(segmentation_dir, seg_files)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0005\n",
    "base_lr = 0.0001\n",
    "max_lr = 0.001\n",
    "batch_size = 16\n",
    "eval_frequency = 10 # number of epochs per eval\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder = autoencoder.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=base_lr, weight_decay=1e-5)\n",
    "scheduler = CyclicLR(\n",
    "    optimizer,\n",
    "    base_lr=base_lr,\n",
    "    max_lr=max_lr,\n",
    "    step_size_up=4,\n",
    "    mode='triangular',\n",
    "    cycle_momentum=False  # Disable cycle_momentum\n",
    ")\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop with test evaluation\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    autoencoder.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        seg_tensor = batch.to(device)\n",
    "        seg_tensor = seg_tensor.float() / 255.0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        encoded, reconstructed = autoencoder(seg_tensor)\n",
    "        loss = criterion(reconstructed, seg_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    # Print epoch statistics\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "      autoencoder.eval()\n",
    "      val_loss = 0\n",
    "      with torch.no_grad():\n",
    "          for batch in test_loader:\n",
    "              seg_tensor = batch.to(device)\n",
    "              encoded, reconstructed = autoencoder(seg_tensor)\n",
    "              loss = criterion(reconstructed, seg_tensor)\n",
    "              val_loss += loss.item()\n",
    "      #         plt.figure(figsize=(10, 5))\n",
    "      #         for i in range(4):\n",
    "      #            plt.subplot(2, 4, i + 1)\n",
    "      #            plt.imshow(seg_tensor[i, 0].cpu().numpy(), cmap='gray')\n",
    "      #            plt.title('Original')\n",
    "      #            plt.axis('off')\n",
    "      #            plt.subplot(2, 4, i + 5)\n",
    "      #            plt.imshow(reconstructed[i, 0].cpu().numpy(), cmap='gray')\n",
    "      #            plt.title('Reconstructed')\n",
    "      #            plt.axis('off')\n",
    "      # plt.show()\n",
    "      test_losses.append(val_loss/len(test_loader))\n",
    "      print(f\"Validation Loss: {val_loss/len(test_loader):.6f}\")\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        checkpoint_path = f'autoencoder_checkpoint_epoch_{epoch+1}_{timestamp}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': autoencoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses - normalize tensor, dynamic learning rate\n",
    "# 0.010598669550978603,\n",
    "#  0.009087359782127912,\n",
    "#  0.004185134204014329,\n",
    "#  0.0036549259360582485,\n",
    "#  0.0034981117865065395,\n",
    "#  0.0033929046035141943,\n",
    "#  0.003267997186604431,\n",
    "#  0.0031347709271441243,\n",
    "#  0.0031136135601218355,\n",
    "#  0.0030799311230176704,\n",
    "#  0.0029925421501199403,\n",
    "#  0.0029478789870348624,\n",
    "#  0.002950479022089254,\n",
    "#  0.002869488670095334,\n",
    "#  0.002872124812472877,\n",
    "#  0.0027697143573420084,\n",
    "#  0.0027105508848462266,\n",
    "#  0.0026728716898481346,\n",
    "#  0.0026305040820207223,\n",
    "#  0.002590992351983305,\n",
    "#  0.0025186698235030104,\n",
    "#  0.0024867024474334175,\n",
    "#  0.0025063614902269635,\n",
    "#  0.002402767481645861,\n",
    "#  0.0023577577461676934,\n",
    "#  0.002360801220449627,\n",
    "#  0.0022824687652417226,\n",
    "#  0.0023271376243858806,\n",
    "#  0.0022554369334879722,\n",
    "#  0.0022152050453346974,\n",
    "#  0.00219290150943719,\n",
    "#  0.0022095415984649223,\n",
    "#  0.0021607008453543453,\n",
    "#  0.0022139052241547112,\n",
    "#  0.0021012918201097396,\n",
    "#  0.0021487404303784426,\n",
    "#  0.0021049527768470196,\n",
    "#  0.002043176349186129,\n",
    "#  0.0021780415015571096,\n",
    "#  0.0020342982106144273,\n",
    "#  0.0020528297162909284,\n",
    "#  0.002011137314345104,\n",
    "#  0.0020772715877248053,\n",
    "#  0.00196996894137711,\n",
    "#  0.001978731192807611,\n",
    "#  0.002019311662264529,\n",
    "#  0.0021013033349449053,\n",
    "#  0.0019783886884832676,\n",
    "#  0.0019796134206198387,\n",
    "#  0.001969414494170189,\n",
    "#  0.001913465900768518,\n",
    "#  0.001958244603241335,\n",
    "#  0.001977357328497935,\n",
    "#  0.0018932805781350856,\n",
    "#  0.001982925513141027,\n",
    "#  0.0019326269619320746,\n",
    "#  0.0018998307940096427,\n",
    "#  0.0019092814302989986,\n",
    "#  0.001899283706672235,\n",
    "#  0.0018762577307858952,\n",
    "#  0.0019800624560745183,\n",
    "#  0.0018605668813473097,\n",
    "#  0.0018876566902530031,\n",
    "#  0.0018942271691927436,\n",
    "#  0.0018847197260405135,\n",
    "#  0.0018821798951200546,\n",
    "#  0.001874544504868063,\n",
    "#  0.0018958703324893898,\n",
    "#  0.001916885769002649,\n",
    "#  0.0018252110996938966,\n",
    "#  0.0018747157100163548,\n",
    "#  0.0018779334510700442,\n",
    "#  0.0018953005942460308,\n",
    "#  0.0018827289892121768,\n",
    "#  0.0018879924637154064,\n",
    "#  0.001826623885194503,\n",
    "#  0.0018644464750140056,\n",
    "#  0.001829887837326775,\n",
    "#  0.001967043435450463,\n",
    "#  0.0018982560956914327,\n",
    "#  0.0018301724260235126,\n",
    "#  0.001860489436137381,\n",
    "#  0.0018488050468645437,\n",
    "#  0.001918041868336987,\n",
    "#  0.0018233922399723758,\n",
    "#  0.0018673976959575741,\n",
    "#  0.0018486381648373698,\n",
    "#  0.001847924745709309,\n",
    "#  0.0017870324852868405,\n",
    "#  0.0019125953823144939,\n",
    "#  0.0018307854515298714,\n",
    "#  0.0018192705097329668,\n",
    "#  0.0018259260439953404,\n",
    "#  0.001847274343422123,\n",
    "#  0.0018665836162154066,\n",
    "#  0.0018549766855850349,\n",
    "#  0.0018079144353819136,\n",
    "#  0.0018595705990231148,\n",
    "#  0.0018406131396340763,\n",
    "#  0.0018173889850698773]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_losses\n",
    "# [674.6184588345615,\n",
    "#  669.6901964707808,\n",
    "#  669.3669828935103,\n",
    "#  669.2363950555974,\n",
    "#  669.2811737060547,\n",
    "#  668.5389347076416,\n",
    "#  668.6799944097346,\n",
    "#  668.8191734660755,\n",
    "#  670.0550876964222,\n",
    "#  668.7530928525058]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # norm tensor, lr = 0.0005\n",
    "##### train loss\n",
    "# [0.012919229262925026,\n",
    "#  0.010495140084577591,\n",
    "#  0.010502334075855182,\n",
    "#  0.010507502728420446,\n",
    "#  0.010485618315541591,\n",
    "#  0.010513703299001751,\n",
    "#  0.010506574628328071,\n",
    "#  0.010474914830162102,\n",
    "#  0.010476896819921259,\n",
    "#  0.010497446879717997,\n",
    "#  0.010483136958445282,\n",
    "#  0.010488326502751633,\n",
    "#  0.010519665383343767,\n",
    "#  0.010484851373391401,\n",
    "#  0.010485852921270153,\n",
    "#  0.010498622436331123,\n",
    "#  0.01049230278514729,\n",
    "#  0.010421480214026918,\n",
    "#  0.010359131299925071,\n",
    "#  0.010377319968780475,\n",
    "#  0.010368558291632395,\n",
    "#  0.010366258009116662,\n",
    "#  0.010376635081853791,\n",
    "#  0.010358538960857566,\n",
    "#  0.010361543597860469,\n",
    "#  0.010378243408279966,\n",
    "#  0.010363919505891933,\n",
    "#  0.010391937849374536,\n",
    "#  0.01036856722078708,\n",
    "#  0.010374640451579691,\n",
    "#  0.010385230904646705,\n",
    "#  0.01036395997995049,\n",
    "#  0.010369975140392567,\n",
    "#  0.010380298144016892,\n",
    "#  0.010357583891547303,\n",
    "#  0.010378568361584958,\n",
    "#  0.010395116816166035,\n",
    "#  0.010365820313376664,\n",
    "#  0.01037341430372004,\n",
    "#  0.010382045142649754,\n",
    "#  0.010369892828458353,\n",
    "#  0.01036810908968059,\n",
    "#  0.010363976306618958,\n",
    "#  0.010389761303410868,\n",
    "#  0.01036483648119022,\n",
    "#  0.010375067760154308,\n",
    "#  0.010382358115458293,\n",
    "#  0.01036999827155318,\n",
    "#  0.01035941466145175,\n",
    "#  0.010365696513144422,\n",
    "#  0.010379880772866414,\n",
    "#  0.010377261351178429,\n",
    "#  0.010380936196164103,\n",
    "#  0.010368056516388585,\n",
    "#  0.01036732699297517,\n",
    "#  0.010383301340919146,\n",
    "#  0.010361180566463546,\n",
    "#  0.010379377396331511,\n",
    "#  0.010371307840443447,\n",
    "#  0.010383967946991961,\n",
    "#  0.010379769261581477,\n",
    "#  0.010364630549450927,\n",
    "#  0.010393445259968397,\n",
    "#  0.010372201000682564,\n",
    "#  0.010361896019211395,\n",
    "#  0.010377790731778554,\n",
    "#  0.01038492830374684,\n",
    "#  0.010386664957667772,\n",
    "#  0.010391447969056942,\n",
    "#  0.010383025772254011,\n",
    "#  0.010374613995783223,\n",
    "#  0.010389668922048815,\n",
    "#  0.010372335946272284,\n",
    "#  0.010381413854531243,\n",
    "#  0.010374731563318234,\n",
    "#  0.010384616603215154,\n",
    "#  0.01036797573609527,\n",
    "#  0.010367941154443004,\n",
    "#  0.010362121386007749,\n",
    "#  0.010378099776316232,\n",
    "#  0.01037243392419314,\n",
    "#  0.010412328782361372,\n",
    "#  0.010378307600550757,\n",
    "#  0.010391010809955093,\n",
    "#  0.010383097464855537,\n",
    "#  0.010361636474070556,\n",
    "#  0.01039646823553533,\n",
    "#  0.01037824592630026,\n",
    "#  0.01038795979653732,\n",
    "#  0.01035960474346056,\n",
    "#  0.010373186517076997,\n",
    "#  0.010373493545739102,\n",
    "#  0.010364184195860328,\n",
    "#  0.010390874065400402,\n",
    "#  0.010368071978067996,\n",
    "#  0.010388104152199719,\n",
    "#  0.010358121579093726,\n",
    "#  0.010385539870910834,\n",
    "#  0.010366724525442999,\n",
    "#  0.010368037610009652]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_losses\n",
    "# [689.2446632385254,\n",
    "#  689.2478457364169,\n",
    "#  689.2478457364169,\n",
    "#  689.2478457364169,\n",
    "#  689.2478457364169,\n",
    "#  689.2478457364169,\n",
    "#  689.2478457364169,\n",
    "#  689.2478457364169,\n",
    "#  689.2478457364169,\n",
    "#  689.2478457364169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### lr = 0.0005, normalize in autoencoder layers\n",
    "# Epoch 1, Train Loss: 652.069466\n",
    "# Training Epoch 2: 100%|█████████████████████████████████████| 351/351 [01:39<00:00,  3.53it/s]\n",
    "# Epoch 2, Train Loss: 652.692586\n",
    "# Training Epoch 3: 100%|█████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 3, Train Loss: 652.600141\n",
    "# Training Epoch 4: 100%|█████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 4, Train Loss: 651.894051\n",
    "# Training Epoch 5: 100%|█████████████████████████████████████| 351/351 [01:39<00:00,  3.55it/s]\n",
    "# Epoch 5, Train Loss: 651.884299\n",
    "# Training Epoch 6: 100%|█████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 6, Train Loss: 651.157909\n",
    "# Training Epoch 7: 100%|█████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 7, Train Loss: 652.808403\n",
    "# Training Epoch 8: 100%|█████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 8, Train Loss: 651.930558\n",
    "# Training Epoch 9: 100%|█████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 9, Train Loss: 652.140237\n",
    "# Training Epoch 10: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.53it/s]\n",
    "# Epoch 10, Train Loss: 651.177875\n",
    "# Validation Loss: 659.643295\n",
    "# Training Epoch 11: 100%|████████████████████████████████████| 351/351 [01:38<00:00,  3.55it/s]\n",
    "# Epoch 11, Train Loss: 652.893720\n",
    "# Training Epoch 12: 100%|████████████████████████████████████| 351/351 [01:38<00:00,  3.55it/s]\n",
    "# Epoch 12, Train Loss: 651.975086\n",
    "# Training Epoch 13: 100%|████████████████████████████████████| 351/351 [01:38<00:00,  3.55it/s]\n",
    "# Epoch 13, Train Loss: 652.931841\n",
    "# Training Epoch 14: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 14, Train Loss: 652.330461\n",
    "# Training Epoch 15: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 15, Train Loss: 653.820897\n",
    "# Training Epoch 16: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 16, Train Loss: 651.770907\n",
    "# Training Epoch 17: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.53it/s]\n",
    "# Epoch 17, Train Loss: 651.725226\n",
    "# Training Epoch 18: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 18, Train Loss: 652.241266\n",
    "# Training Epoch 19: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 19, Train Loss: 651.556436\n",
    "# Training Epoch 20: 100%|████████████████████████████████████| 351/351 [01:38<00:00,  3.55it/s]\n",
    "# Epoch 20, Train Loss: 651.854807\n",
    "# Validation Loss: 659.645702\n",
    "# Training Epoch 21: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 21, Train Loss: 651.743324\n",
    "# Training Epoch 22: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.55it/s]\n",
    "# Epoch 22, Train Loss: 652.704887\n",
    "# Training Epoch 23: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.53it/s]\n",
    "# Epoch 23, Train Loss: 652.056867\n",
    "# Training Epoch 24: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.53it/s]\n",
    "# Epoch 24, Train Loss: 652.186908\n",
    "# Training Epoch 25: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 25, Train Loss: 652.562346\n",
    "# Training Epoch 26: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 26, Train Loss: 652.013083\n",
    "# Training Epoch 27: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 27, Train Loss: 652.361789\n",
    "# Training Epoch 28: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 28, Train Loss: 651.564174\n",
    "# Training Epoch 29: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 29, Train Loss: 651.654328\n",
    "# Training Epoch 30: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 30, Train Loss: 651.677792\n",
    "# Validation Loss: 659.642267\n",
    "# Training Epoch 31: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 31, Train Loss: 652.280122\n",
    "# Training Epoch 32: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 32, Train Loss: 652.841016\n",
    "# Training Epoch 33: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.53it/s]\n",
    "# Epoch 33, Train Loss: 652.137129\n",
    "# Training Epoch 34: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 34, Train Loss: 652.149949\n",
    "# Training Epoch 35: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.53it/s]\n",
    "# Epoch 35, Train Loss: 651.757967\n",
    "# Training Epoch 36: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 36, Train Loss: 653.631322\n",
    "# Training Epoch 37: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 37, Train Loss: 652.113598\n",
    "# Training Epoch 38: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 38, Train Loss: 651.450653\n",
    "# Training Epoch 39: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 39, Train Loss: 652.112651\n",
    "# Training Epoch 40: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 40, Train Loss: 653.152022\n",
    "# Validation Loss: 659.641706\n",
    "# Training Epoch 41: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 41, Train Loss: 651.838199\n",
    "# Training Epoch 42: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 42, Train Loss: 653.216685\n",
    "# Training Epoch 43: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 43, Train Loss: 651.788312\n",
    "# Training Epoch 44: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.55it/s]\n",
    "# Epoch 44, Train Loss: 652.100813\n",
    "# Training Epoch 45: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 45, Train Loss: 651.938989\n",
    "# Training Epoch 46: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.53it/s]\n",
    "# Epoch 46, Train Loss: 652.252912\n",
    "# Training Epoch 47: 100%|████████████████████████████████████| 351/351 [01:38<00:00,  3.55it/s]\n",
    "# Epoch 47, Train Loss: 652.736892\n",
    "# Training Epoch 48: 100%|████████████████████████████████████| 351/351 [01:39<00:00,  3.54it/s]\n",
    "# Epoch 48, Train Loss: 652.466110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lr = 0.005, no normalization\n",
    "# Training Epoch 1: 100%|█████████████████████████████████████| 351/351 [02:08<00:00,  2.74it/s]\n",
    "# Epoch 1, Train Loss: 656.253363\n",
    "# Training Epoch 2: 100%|█████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 2, Train Loss: 657.944037\n",
    "# Training Epoch 3: 100%|█████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 3, Train Loss: 656.778741\n",
    "# Training Epoch 4: 100%|█████████████████████████████████████| 351/351 [01:37<00:00,  3.58it/s]\n",
    "# Epoch 4, Train Loss: 656.744963\n",
    "# Training Epoch 5: 100%|█████████████████████████████████████| 351/351 [01:37<00:00,  3.58it/s]\n",
    "# Epoch 5, Train Loss: 656.823661\n",
    "# Training Epoch 6: 100%|█████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 6, Train Loss: 660.347832\n",
    "# Training Epoch 7: 100%|█████████████████████████████████████| 351/351 [01:37<00:00,  3.60it/s]\n",
    "# Epoch 7, Train Loss: 656.921275\n",
    "# Training Epoch 8: 100%|█████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 8, Train Loss: 656.122115\n",
    "# Training Epoch 9: 100%|█████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 9, Train Loss: 656.077839\n",
    "# Training Epoch 10: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 10, Train Loss: 656.742585\n",
    "# Validation Loss: 639.907631\n",
    "# Training Epoch 11: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 11, Train Loss: 656.353283\n",
    "# Training Epoch 12: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 12, Train Loss: 656.320721\n",
    "# Training Epoch 13: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 13, Train Loss: 657.266287\n",
    "# Training Epoch 14: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 14, Train Loss: 656.678734\n",
    "# Training Epoch 15: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 15, Train Loss: 657.124233\n",
    "# Training Epoch 16: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.58it/s]\n",
    "# Epoch 16, Train Loss: 656.573825\n",
    "# Training Epoch 17: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.60it/s]\n",
    "# Epoch 17, Train Loss: 657.191130\n",
    "# Training Epoch 18: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 18, Train Loss: 656.645940\n",
    "# Training Epoch 19: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 19, Train Loss: 657.434529\n",
    "# Training Epoch 20: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 20, Train Loss: 657.011962\n",
    "# Validation Loss: 639.907631\n",
    "# Training Epoch 21: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.60it/s]\n",
    "# Epoch 21, Train Loss: 658.020220\n",
    "# Training Epoch 22: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 22, Train Loss: 657.597698\n",
    "# Training Epoch 23: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 23, Train Loss: 656.368929\n",
    "# Training Epoch 24: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 24, Train Loss: 656.939704\n",
    "# Training Epoch 25: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 25, Train Loss: 656.370394\n",
    "# Training Epoch 26: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 26, Train Loss: 656.970797\n",
    "# Training Epoch 27: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 27, Train Loss: 655.919122\n",
    "# Training Epoch 28: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 28, Train Loss: 657.775658\n",
    "# Training Epoch 29: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 29, Train Loss: 656.966953\n",
    "# Training Epoch 30: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.59it/s]\n",
    "# Epoch 30, Train Loss: 657.083070\n",
    "# Validation Loss: 639.907631\n",
    "# Training Epoch 31: 100%|████████████████████████████████████| 351/351 [01:37<00:00,  3.60it/s]\n",
    "# Epoch 31, Train Loss: 656.580937\n",
    "# Training Epoch 32:  17%|██████▍                              | 61/351 [00:17<01:22,  3.54it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RymPsyokaa5Q"
   },
   "outputs": [],
   "source": [
    "torch.save(trained_autoencoder.state_dict(), 'trained_autoencoder_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskAutoencoder2(nn.Module):\n",
    "    def __init__(self, height, width, k):\n",
    "        super(MaskAutoencoder2, self).__init__()\n",
    "        \n",
    "        # Improved encoder with residual connections and proper normalization\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Calculate encoded dimensions\n",
    "        def conv_output_size(input_size, kernel_size=3, stride=2, padding=1):\n",
    "            return (input_size + 2*padding - kernel_size) // stride + 1\n",
    "            \n",
    "        h_after_conv1 = conv_output_size(height)\n",
    "        w_after_conv1 = conv_output_size(width)\n",
    "        h_after_conv2 = conv_output_size(h_after_conv1)\n",
    "        w_after_conv2 = conv_output_size(w_after_conv1)\n",
    "        \n",
    "        self.encoded_dim = h_after_conv2 * w_after_conv2 * 128\n",
    "        \n",
    "        # Improved bottleneck\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.encoded_dim, k * 2)\n",
    "        self.fc2 = nn.Linear(k * 2, k)\n",
    "        self.fc3 = nn.Linear(k, k * 2)\n",
    "        self.fc4 = nn.Linear(k * 2, self.encoded_dim)\n",
    "        \n",
    "        # Improved decoder with skip connections\n",
    "        self.unflatten = nn.Unflatten(1, (128, h_after_conv2, w_after_conv2))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        return self.fc2(x)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        x = F.leaky_relu(self.fc3(z), 0.2)\n",
    "        x = F.leaky_relu(self.fc4(x), 0.2)\n",
    "        x = self.unflatten(x)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        reconstructed = self.decode(z)\n",
    "        reconstructed = F.interpolate(reconstructed, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n",
    "        return z, reconstructed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 648  # Encoded dimension size\n",
    "autoencoder2 = MaskAutoencoder2(height=270, width=480, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "segmentation_dir = 'segmentation_input/extracted_masks'\n",
    "seg_files = [f for f in os.listdir(segmentation_dir)]\n",
    "full_dataset = SegmentationDataset(segmentation_dir, seg_files)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0005\n",
    "base_lr = 0.0001\n",
    "max_lr = 0.001\n",
    "batch_size = 16\n",
    "eval_frequency = 10 # number of epochs per eval\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder2 = autoencoder2.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(autoencoder2.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(autoencoder2.parameters(), lr=base_lr, weight_decay=1e-5)\n",
    "scheduler = CyclicLR(\n",
    "    optimizer,\n",
    "    base_lr=base_lr,\n",
    "    max_lr=max_lr,\n",
    "    step_size_up=4,\n",
    "    mode='triangular',\n",
    "    cycle_momentum=False  # Disable cycle_momentum\n",
    ")\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop with test evaluation\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    autoencoder2.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        seg_tensor = batch.to(device)\n",
    "        seg_tensor = seg_tensor.float() / 255.0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        encoded, reconstructed = autoencoder2(seg_tensor)\n",
    "        loss = criterion(reconstructed, seg_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    # Print epoch statistics\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "      autoencoder2.eval()\n",
    "      val_loss = 0\n",
    "      with torch.no_grad():\n",
    "          for batch in test_loader:\n",
    "              seg_tensor = batch.to(device)\n",
    "              seg_tensor = seg_tensor.float() / 255.0\n",
    "              encoded, reconstructed = autoencoder2(seg_tensor)\n",
    "              loss = criterion(reconstructed, seg_tensor)\n",
    "              val_loss += loss.item()\n",
    "      #         plt.figure(figsize=(10, 5))\n",
    "      #         for i in range(4):\n",
    "      #            plt.subplot(2, 4, i + 1)\n",
    "      #            plt.imshow(seg_tensor[i, 0].cpu().numpy(), cmap='gray')\n",
    "      #            plt.title('Original')\n",
    "      #            plt.axis('off')\n",
    "      #            plt.subplot(2, 4, i + 5)\n",
    "      #            plt.imshow(reconstructed[i, 0].cpu().numpy(), cmap='gray')\n",
    "      #            plt.title('Reconstructed')\n",
    "      #            plt.axis('off')\n",
    "      # plt.show()\n",
    "      test_losses.append(val_loss/len(test_loader))\n",
    "      print(f\"Validation Loss: {val_loss/len(test_loader):.6f}\")\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        checkpoint_path = f'autoencoder2_checkpoint_epoch_{epoch+1}_{timestamp}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': autoencoder2.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
